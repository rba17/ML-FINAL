{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acfb76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.4.2)\n",
      "Requirement already satisfied: keras in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.3.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.16.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ac5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,cross_validate,StratifiedKFold\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report,roc_auc_score\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c38041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Husse\\AppData\\Local\\Temp\\ipykernel_12936\\4014827458.py:1: DtypeWarning: Columns (662,664,676,677,683,685,686,687) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cancer_type</th>\n",
       "      <th>age_at_diagnosis</th>\n",
       "      <th>cellularity</th>\n",
       "      <th>chemotherapy</th>\n",
       "      <th>pam50_+_claudin-low_subtype</th>\n",
       "      <th>cohort</th>\n",
       "      <th>er_status</th>\n",
       "      <th>neoplasm_histologic_grade</th>\n",
       "      <th>her2_status_measured_by_snp6</th>\n",
       "      <th>her2_status</th>\n",
       "      <th>...</th>\n",
       "      <th>mtap_mut</th>\n",
       "      <th>ppp2cb_mut</th>\n",
       "      <th>smarcd1_mut</th>\n",
       "      <th>nras_mut</th>\n",
       "      <th>ndfip1_mut</th>\n",
       "      <th>hras_mut</th>\n",
       "      <th>prps2_mut</th>\n",
       "      <th>smarcb1_mut</th>\n",
       "      <th>stmn2_mut</th>\n",
       "      <th>siah1_mut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>54.29</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>LumB</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>43.45</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0</td>\n",
       "      <td>LumA</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>74.11</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>LumB</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>51.87</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>LumA</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>87.18</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0</td>\n",
       "      <td>LumB</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GAIN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 686 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cancer_type  age_at_diagnosis cellularity  \\\n",
       "0  Breast Invasive Ductal Carcinoma             54.29        High   \n",
       "1  Breast Invasive Ductal Carcinoma             43.45    Moderate   \n",
       "2  Breast Invasive Ductal Carcinoma             74.11        High   \n",
       "3  Breast Invasive Ductal Carcinoma             51.87        High   \n",
       "4  Breast Invasive Ductal Carcinoma             87.18    Moderate   \n",
       "\n",
       "   chemotherapy pam50_+_claudin-low_subtype  cohort er_status  \\\n",
       "0             1                        LumB       1  Positive   \n",
       "1             0                        LumA       4  Positive   \n",
       "2             0                        LumB       3  Positive   \n",
       "3             0                        LumA       3  Positive   \n",
       "4             0                        LumB       1  Positive   \n",
       "\n",
       "   neoplasm_histologic_grade her2_status_measured_by_snp6 her2_status  ...  \\\n",
       "0                        3.0                      NEUTRAL    Negative  ...   \n",
       "1                        1.0                         LOSS    Negative  ...   \n",
       "2                        3.0                      NEUTRAL    Negative  ...   \n",
       "3                        2.0                      NEUTRAL    Negative  ...   \n",
       "4                        3.0                         GAIN    Positive  ...   \n",
       "\n",
       "   mtap_mut ppp2cb_mut smarcd1_mut  nras_mut  ndfip1_mut  hras_mut  prps2_mut  \\\n",
       "0         0          0           0         0           0         0          0   \n",
       "1         0          0           0         0           0         0          0   \n",
       "2         0          0           0         0           0         0          0   \n",
       "3         0          0           0         0           0         0          0   \n",
       "4         0          0           0         0           0         0          0   \n",
       "\n",
       "   smarcb1_mut stmn2_mut  siah1_mut  \n",
       "0            0         0          0  \n",
       "1            0         0          0  \n",
       "2            0         0          0  \n",
       "3            0         0          0  \n",
       "4            0         0          0  \n",
       "\n",
       "[5 rows x 686 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data_cleaned = data.drop(columns=['patient_id','er_status_measured_by_ihc'])\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ea3f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_with_mode(data):\n",
    "    # Get a list of columns with missing values\n",
    "    columns_with_missing_values = data.columns[data.isnull().any()].tolist()\n",
    "    \n",
    "    # Iterate over each column with missing values\n",
    "    for column_name in columns_with_missing_values:\n",
    "        # Loop until no more null values in the column\n",
    "        while data[column_name].isnull().any():\n",
    "            # Iterate over rows in the DataFrame\n",
    "            for i, row in data[data[column_name].isnull()].iterrows():\n",
    "                # Filter the data for the same cancer type\n",
    "                same_type_data = data[data['cancer_type'] == row['cancer_type']]\n",
    "                \n",
    "                # Try to find 5 other entries; if fewer, take as many as available\n",
    "                if len(same_type_data) > 5:\n",
    "                    sample = same_type_data.sample(n=5)\n",
    "                else:\n",
    "                    sample = same_type_data\n",
    "                \n",
    "                # Calculate the mode of the selected sample\n",
    "                mode_value = sample[column_name].mode()\n",
    "                \n",
    "                # If mode calculation is successful and not empty, use the mode to fill the missing value\n",
    "                if not mode_value.empty:\n",
    "                    data.at[i, column_name] = mode_value.iloc[0]\n",
    "                else:\n",
    "                    # If no mode available (all values are different or no other samples), we might choose to do nothing or use a global mode\n",
    "                    # Here we're choosing to use the global mode as a fallback\n",
    "                    global_mode = data[column_name].mode()[0]\n",
    "                    data.at[i, column_name] = global_mode\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage for the column 'cellularity'\n",
    "updated_data = fill_missing_with_mode(data_cleaned)\n",
    "\n",
    "missing_values = updated_data.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Display columns with missing values and their counts\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06029b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in X after encoding: 734\n"
     ]
    }
   ],
   "source": [
    "X = data_cleaned.drop(columns=['cancer_type'])\n",
    "y = data_cleaned['cancer_type']\n",
    "\n",
    "cols_one_hot = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() <= 5]\n",
    "cols_label = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() > 5]\n",
    "\n",
    "# Initialize the transformers list for the ColumnTransformer\n",
    "transformers = []\n",
    "\n",
    "# Loop through each column in X to apply appropriate encoding\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object' or X[column].dtype == 'int':  # Adjusted to ensure we catch int types too\n",
    "        X[column] = X[column].astype(str)  # Convert everything to string to avoid mixed type errors\n",
    "        unique_values = X[column].nunique()\n",
    "        if unique_values > 5:\n",
    "            # Use LabelEncoder for columns with more than 5 unique values\n",
    "            transformers.append((column, LabelEncoder(), [column]))  # LabelEncoder usage adjusted\n",
    "        else:\n",
    "            # Use OneHotEncoder for columns with 5 or fewer unique values\n",
    "            transformers.append((column, OneHotEncoder(), [column]))\n",
    "\n",
    "# Manually apply LabelEncoder to the relevant columns before ColumnTransformer\n",
    "for name, encoder, columns in transformers:\n",
    "    if isinstance(encoder, LabelEncoder):\n",
    "        X[columns[0]] = encoder.fit_transform(X[columns[0]])  # Directly encode the column in the DataFrame\n",
    "        # Remove label encoded columns from transformer list since they are already processed\n",
    "        transformers = [(n, e, c) for n, e, c in transformers if e is not LabelEncoder]\n",
    "\n",
    "# Setup remaining transformations with OneHotEncoder using ColumnTransformer\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), [name for name, encoder, _ in transformers if isinstance(encoder, OneHotEncoder)])\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep all other columns that do not need encoding\n",
    ")\n",
    "\n",
    "# Fit and transform X with the defined ColumnTransformer for OneHotEncoder\n",
    "X_transformed = preprocessor_X.fit_transform(X)\n",
    "\n",
    "# Label encode y\n",
    "label_encoder_y = LabelEncoder()\n",
    "y_encoded = label_encoder_y.fit_transform(y)\n",
    "\n",
    "# Print the final shape of X_transformed to verify feature count\n",
    "print(\"Number of features in X after encoding:\", X_transformed.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660b1206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names and counts are consistent.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ahnak2_mut</th>\n",
       "      <th>kmt2c_mut</th>\n",
       "      <th>syne1_mut</th>\n",
       "      <th>gata3_mut</th>\n",
       "      <th>map3k1_mut</th>\n",
       "      <th>ahnak_mut</th>\n",
       "      <th>dnah11_mut</th>\n",
       "      <th>cdh1_mut</th>\n",
       "      <th>dnah2_mut</th>\n",
       "      <th>kmt2d_mut</th>\n",
       "      <th>...</th>\n",
       "      <th>spry2</th>\n",
       "      <th>srd5a1</th>\n",
       "      <th>srd5a2</th>\n",
       "      <th>srd5a3</th>\n",
       "      <th>st7</th>\n",
       "      <th>star</th>\n",
       "      <th>tnk2</th>\n",
       "      <th>tulp4</th>\n",
       "      <th>ugt2b15</th>\n",
       "      <th>ugt2b17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.160</td>\n",
       "      <td>111.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.046</td>\n",
       "      <td>76.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.056</td>\n",
       "      <td>118.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.028</td>\n",
       "      <td>220.233333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.052</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 658 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ahnak2_mut  kmt2c_mut  syne1_mut  gata3_mut  map3k1_mut   ahnak_mut  \\\n",
       "0         3.0        1.0        3.0        2.0       5.160  111.100000   \n",
       "1         1.0        1.0        0.0        7.0       2.046   76.866667   \n",
       "2         3.0        1.0        6.0        3.0       6.056  118.700000   \n",
       "3         2.0        0.0        0.0       10.0       3.028  220.233333   \n",
       "4         3.0        1.0        2.0        1.0       5.052   28.600000   \n",
       "\n",
       "   dnah11_mut  cdh1_mut  dnah2_mut  kmt2d_mut  ...  spry2  srd5a1  srd5a2  \\\n",
       "0         0.0       1.0       80.0        3.0  ...    0.0     0.0     0.0   \n",
       "1         1.0       1.0       23.0        2.0  ...    0.0     0.0     0.0   \n",
       "2         0.0       1.0       28.0        2.0  ...    0.0     0.0     0.0   \n",
       "3         1.0       1.0       14.0        1.0  ...    0.0     0.0     0.0   \n",
       "4         0.0       0.0       26.0        2.0  ...    0.0     0.0     0.0   \n",
       "\n",
       "   srd5a3  st7  star  tnk2  tulp4  ugt2b15  ugt2b17  \n",
       "0     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "1     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "2     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "3     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "4     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 658 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Assume cols_label and cols_one_hot are defined and scoped correctly from a previous cell\n",
    "onehot_features = preprocessor_X.named_transformers_['onehot'].get_feature_names_out()\n",
    "label_features = cols_label  # Using the columns designated for label encoding\n",
    "remainder_features = [col for col in X.columns if col not in cols_one_hot and col not in cols_label]\n",
    "\n",
    "# Combining all feature names\n",
    "all_features = list(onehot_features) + label_features + remainder_features\n",
    "\n",
    "# Create the DataFrame from the transformed data\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns=all_features[:X_transformed.shape[1]])\n",
    "\n",
    "# Verify column counts and consistency\n",
    "if len(all_features[:X_transformed.shape[1]]) != X_transformed_df.shape[1]:\n",
    "    print(f\"Warning: Column count mismatch. {len(all_features[:X_transformed.shape[1]])} names for {X_transformed_df.shape[1]} actual columns.\")\n",
    "else:\n",
    "    print(\"Column names and counts are consistent.\")\n",
    "\n",
    "# Find the index of 'muc16_mut' column\n",
    "muc16_mut_index = X_transformed_df.columns.get_loc(\"muc16_mut\")\n",
    "\n",
    "# Select columns after 'muc16_mut'\n",
    "selected_columns = X_transformed_df.columns[muc16_mut_index + 1:]\n",
    "\n",
    "# Display the selected columns for the first 30 rows of the DataFrame\n",
    "display(X_transformed_df[selected_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e5bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      brca1     brca2     palb2      pten      tp53       atm      cdh1  \\\n",
      "0 -1.211576 -0.109631 -0.594727 -1.140829  0.595648 -1.106277 -2.143642   \n",
      "1  0.427194 -0.010864 -0.736981  0.064916  0.152814  0.097883  0.103718   \n",
      "2 -0.477025 -0.151244 -0.772095 -0.847137  0.240765 -0.248629 -0.571054   \n",
      "3 -0.548368  0.201905 -0.655579 -0.891006 -0.371917  1.477406  0.814926   \n",
      "4  0.972271  0.990202 -0.241689  0.815784  0.212176  0.843807 -0.182637   \n",
      "\n",
      "      chek2       nbn       nf1  ...     spry2    srd5a1    srd5a2    srd5a3  \\\n",
      "0 -1.105545 -0.703869 -0.394923  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "1 -0.322918  0.518080 -0.696819  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "2  0.063015  0.116458  0.202719  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "3  0.903104  0.189407 -0.800239  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "4 -0.134838 -0.517778  1.051354  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "\n",
      "        st7      star      tnk2     tulp4   ugt2b15   ugt2b17  \n",
      "0 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "1 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "2 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "3 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "4 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "\n",
      "[5 rows x 488 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_index = X_transformed_df.columns.get_loc(\"brca1\")\n",
    "end_index = X_transformed_df.columns.get_loc(\"ugt2b17\") +1\n",
    "\n",
    "# Select the columns for scaling\n",
    "columns_to_scale = X_transformed_df.columns[start_index:end_index]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the selected columns\n",
    "X_transformed_df[columns_to_scale] = scaler.fit_transform(X_transformed_df[columns_to_scale])\n",
    "\n",
    "# Display the scaled data for these columns\n",
    "print(X_transformed_df[columns_to_scale].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5254402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (1182, 734) (1182,)\n",
      "Validation set size: (148, 734) (148,)\n",
      "Test set size: (166, 734) (166,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume X_transformed_df and y_encoded are your features and labels respectively\n",
    "\n",
    "# First, isolate all Class 2 instances\n",
    "class_2_indices = (y_encoded == 2)\n",
    "X_class_2 = X_transformed_df[class_2_indices]\n",
    "y_class_2 = y_encoded[class_2_indices]\n",
    "\n",
    "# Remove Class 2 instances from the original dataset\n",
    "X_non_class_2 = X_transformed_df[~class_2_indices]\n",
    "y_non_class_2 = y_encoded[~class_2_indices]\n",
    "\n",
    "# Split the non-Class 2 data into training and remaining (validation + test) sets\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "    X_non_class_2, y_non_class_2, train_size=0.8, random_state=42, stratify=y_non_class_2\n",
    ")\n",
    "\n",
    "# Split the remaining non-Class 2 data into validation and test sets\n",
    "X_valid, X_test_non_class_2, y_valid, y_test_non_class_2 = train_test_split(\n",
    "    X_remaining, y_remaining, test_size=0.5, random_state=42, stratify=y_remaining\n",
    ")\n",
    "\n",
    "# Combine the non-Class 2 test instances with all Class 2 instances to form the final test set\n",
    "X_test = np.concatenate((X_test_non_class_2, X_class_2))\n",
    "y_test = np.concatenate((y_test_non_class_2, y_class_2))\n",
    "\n",
    "# Print the sizes of each dataset to confirm the splits\n",
    "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set size:\", X_valid.shape, y_valid.shape)\n",
    "print(\"Test set size:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd90917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Invasive Ductal Carcinoma             959\n",
      "Breast Mixed Ductal and Lobular Carcinoma    132\n",
      "Breast Invasive Lobular Carcinoma             91\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train to a pandas Series for easy counting\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "# Get counts of each class\n",
    "class_counts_train = y_train_series.value_counts()\n",
    "\n",
    "# Print the counts with original class names using inverse transform of LabelEncoder\n",
    "class_names_counts_train = pd.Series(class_counts_train.index).apply(\n",
    "    lambda x: label_encoder_y.inverse_transform([x])[0])\n",
    "class_counts_train.index = class_names_counts_train\n",
    "\n",
    "# Display the class counts with names\n",
    "print(class_counts_train)\n",
    "\n",
    "# Class Number 0: Breast Invasive Ductal Carcinoma\n",
    "# Class Number 1: Breast Invasive Lobular Carcinoma\n",
    "# Class Number 2: Breast Invasive Mixed Mucinous Carcinoma\n",
    "# Class Number 3: Breast Mixed Ductal and Lobular Carcinoma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca7f19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight(\n",
    "#     class_weight='balanced',\n",
    "#     classes=np.unique(y_train),\n",
    "#     y=y_train\n",
    "# )\n",
    "# class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0873bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.layers import Dropout\n",
    "# from keras.regularizers import l2\n",
    "# from keras.layers import BatchNormalization\n",
    "# num_classes = len(np.unique(y_train))\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense(900, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Set the learning rate\n",
    "# learning_rate = 0.01  # You can change this value according to your needs\n",
    "\n",
    "# # Instantiate the optimizer with the desired learning rate\n",
    "# adam_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# # Compile the model with the customized optimizer\n",
    "# model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e2c4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=600,\n",
    "#                     validation_data=(X_valid, y_valid),\n",
    "#                     class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b47a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Summarize history for accuracy\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Summarize history for loss\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59699c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_valid)\n",
    "\n",
    "# # Convert the predicted probabilities to class labels\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# # Generate the classification report\n",
    "# report = classification_report(y_valid, y_pred_labels)\n",
    "\n",
    "# # Print the classification report\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48a974e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\husse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4fe4d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution: [500  91   0 132]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train contains the class labels, and X_train contains the features\n",
    "\n",
    "# Setup the RandomUnderSampler to reduce class 0 to 500 instances\n",
    "rus = RandomUnderSampler(sampling_strategy={0: 500})\n",
    "\n",
    "# Apply the resampling to the training data\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution to ensure it is as expected\n",
    "print(\"New class distribution:\", np.bincount(y_resampled))\n",
    "\n",
    "# Now X_resampled and y_resampled contain the undersampled data where class 0 has exactly 500 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d899217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.6997 - loss: 3.2654 - val_accuracy: 0.2568 - val_loss: 1.2228\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3648 - loss: 1.5959 - val_accuracy: 0.2365 - val_loss: 0.9916\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3028 - loss: 1.4955 - val_accuracy: 0.3919 - val_loss: 0.7500\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4019 - loss: 1.4346 - val_accuracy: 0.4797 - val_loss: 0.7138\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4077 - loss: 1.3453 - val_accuracy: 0.4527 - val_loss: 0.7952\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4252 - loss: 1.3046 - val_accuracy: 0.5946 - val_loss: 0.7133\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4990 - loss: 1.2376 - val_accuracy: 0.6486 - val_loss: 0.6821\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5239 - loss: 1.1937 - val_accuracy: 0.5541 - val_loss: 0.7211\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5747 - loss: 1.1618 - val_accuracy: 0.5811 - val_loss: 0.6994\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5646 - loss: 1.0307 - val_accuracy: 0.6081 - val_loss: 0.6144\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6593 - loss: 1.0365 - val_accuracy: 0.6284 - val_loss: 0.6279\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6583 - loss: 0.9167 - val_accuracy: 0.7162 - val_loss: 0.5498\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7500 - loss: 0.8422 - val_accuracy: 0.6351 - val_loss: 0.6625\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7442 - loss: 0.7657 - val_accuracy: 0.6216 - val_loss: 0.6847\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7404 - loss: 0.7002 - val_accuracy: 0.7365 - val_loss: 0.4933\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8124 - loss: 0.6248 - val_accuracy: 0.7230 - val_loss: 0.5425\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8400 - loss: 0.5220 - val_accuracy: 0.7230 - val_loss: 0.5722\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8793 - loss: 0.4514 - val_accuracy: 0.7230 - val_loss: 0.5983\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8853 - loss: 0.4211 - val_accuracy: 0.6959 - val_loss: 0.6508\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8999 - loss: 0.3920 - val_accuracy: 0.6959 - val_loss: 0.7175\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9105 - loss: 0.3359 - val_accuracy: 0.7162 - val_loss: 0.7675\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9082 - loss: 0.2930 - val_accuracy: 0.7568 - val_loss: 0.7078\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9383 - loss: 0.2715 - val_accuracy: 0.7568 - val_loss: 0.7593\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9549 - loss: 0.1770 - val_accuracy: 0.7365 - val_loss: 0.7854\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9542 - loss: 0.1552 - val_accuracy: 0.7838 - val_loss: 0.8312\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9796 - loss: 0.1116 - val_accuracy: 0.6622 - val_loss: 1.0724\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9410 - loss: 0.1860 - val_accuracy: 0.7905 - val_loss: 0.8898\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9756 - loss: 0.0986 - val_accuracy: 0.7905 - val_loss: 0.9013\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9789 - loss: 0.1527 - val_accuracy: 0.7027 - val_loss: 0.9751\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9834 - loss: 0.0820 - val_accuracy: 0.7297 - val_loss: 0.9814\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9753 - loss: 0.0768 - val_accuracy: 0.7905 - val_loss: 1.0077\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9941 - loss: 0.0720 - val_accuracy: 0.7162 - val_loss: 1.0022\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9811 - loss: 0.0570 - val_accuracy: 0.7432 - val_loss: 0.9965\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9855 - loss: 0.0776 - val_accuracy: 0.7162 - val_loss: 1.0884\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9830 - loss: 0.0761 - val_accuracy: 0.7297 - val_loss: 1.0775\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9857 - loss: 0.0507 - val_accuracy: 0.7230 - val_loss: 1.0692\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9974 - loss: 0.0277 - val_accuracy: 0.7432 - val_loss: 1.0727\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9933 - loss: 0.0304 - val_accuracy: 0.7297 - val_loss: 1.1218\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9951 - loss: 0.0259 - val_accuracy: 0.7568 - val_loss: 1.1832\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - loss: 0.0183 - val_accuracy: 0.7230 - val_loss: 1.2382\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9975 - loss: 0.0207 - val_accuracy: 0.7297 - val_loss: 1.2596\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9951 - loss: 0.0197 - val_accuracy: 0.7230 - val_loss: 1.2516\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9938 - loss: 0.0192 - val_accuracy: 0.7297 - val_loss: 1.2454\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9978 - loss: 0.0284 - val_accuracy: 0.7095 - val_loss: 1.2860\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9933 - loss: 0.0231 - val_accuracy: 0.7838 - val_loss: 1.2996\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9950 - loss: 0.0266 - val_accuracy: 0.7230 - val_loss: 1.2695\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9950 - loss: 0.0555 - val_accuracy: 0.7432 - val_loss: 1.2870\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9970 - loss: 0.0193 - val_accuracy: 0.7162 - val_loss: 1.2834\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0108 - val_accuracy: 0.7230 - val_loss: 1.2928\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.7568 - val_loss: 1.2816\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9988 - loss: 0.0106 - val_accuracy: 0.7365 - val_loss: 1.3217\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9975 - loss: 0.0162 - val_accuracy: 0.7568 - val_loss: 1.3308\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9979 - loss: 0.0130 - val_accuracy: 0.7635 - val_loss: 1.3354\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9981 - loss: 0.0062 - val_accuracy: 0.7500 - val_loss: 1.3347\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0066 - val_accuracy: 0.7635 - val_loss: 1.3553\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.7703 - val_loss: 1.3894\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.7568 - val_loss: 1.4121\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9953 - loss: 0.0257 - val_accuracy: 0.7230 - val_loss: 1.4262\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9973 - loss: 0.0064 - val_accuracy: 0.7297 - val_loss: 1.3666\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9977 - loss: 0.0132 - val_accuracy: 0.7230 - val_loss: 1.3661\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9958 - loss: 0.0092 - val_accuracy: 0.7432 - val_loss: 1.3809\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.7365 - val_loss: 1.4253\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9990 - loss: 0.0046 - val_accuracy: 0.7297 - val_loss: 1.4695\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.7500 - val_loss: 1.4788\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7432 - val_loss: 1.4946\n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9988 - loss: 0.0068 - val_accuracy: 0.7500 - val_loss: 1.4929\n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0037 - val_accuracy: 0.7635 - val_loss: 1.4907\n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7568 - val_loss: 1.5052\n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.7568 - val_loss: 1.4814\n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7500 - val_loss: 1.4805\n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9976 - loss: 0.0210 - val_accuracy: 0.7095 - val_loss: 1.5239\n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 0.7230 - val_loss: 1.5059\n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9976 - loss: 0.0099 - val_accuracy: 0.7162 - val_loss: 1.5412\n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9978 - loss: 0.0056 - val_accuracy: 0.7162 - val_loss: 1.5870\n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7230 - val_loss: 1.5769\n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9996 - loss: 0.0045 - val_accuracy: 0.7297 - val_loss: 1.5690\n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0044 - val_accuracy: 0.6959 - val_loss: 1.5971\n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9976 - loss: 0.0106 - val_accuracy: 0.7095 - val_loss: 1.5909\n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.0181 - val_accuracy: 0.7770 - val_loss: 1.6566\n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0286 - val_accuracy: 0.6892 - val_loss: 1.8276\n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9906 - loss: 0.0299 - val_accuracy: 0.7838 - val_loss: 1.5660\n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0151 - val_accuracy: 0.6959 - val_loss: 1.5940\n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9938 - loss: 0.0184 - val_accuracy: 0.7432 - val_loss: 1.4898\n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9994 - loss: 0.0048 - val_accuracy: 0.7770 - val_loss: 1.4801\n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.7432 - val_loss: 1.4412\n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.7027 - val_loss: 1.5170\n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.7095 - val_loss: 1.5527\n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 0.7365 - val_loss: 1.5556\n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7432 - val_loss: 1.6002\n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7027 - val_loss: 1.6388\n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7162 - val_loss: 1.6242\n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7095 - val_loss: 1.5832\n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9991 - loss: 0.0021 - val_accuracy: 0.7432 - val_loss: 1.5765\n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7635 - val_loss: 1.5936\n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7568 - val_loss: 1.5872\n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 7.4763e-04 - val_accuracy: 0.7432 - val_loss: 1.5853\n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7027 - val_loss: 1.6276\n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 0.7095 - val_loss: 1.6381\n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7365 - val_loss: 1.6045\n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0043 - val_accuracy: 0.7162 - val_loss: 1.6384\n"
     ]
    }
   ],
   "source": [
    "y_train_binary = (y_train == 0).astype(int)  # 1 for 'Breast Invasive Ductal Carcinoma', 0 otherwise\n",
    "y_valid_binary = (y_valid == 0).astype(int)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Define class weights for balancing\n",
    "class_weights = {\n",
    "    0: 7,  # for 'Other Classes'\n",
    "    1: 1   # for 'Breast Invasive Ductal Carcinoma'\n",
    "}\n",
    "\n",
    "# Model construction with simplified architecture\n",
    "model_class_0 = Sequential([\n",
    "    Dense(250, activation='relu'),\n",
    "    Dropout(0.2),# Reduced number of neurons\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "model_class_0.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with adjusted parameters\n",
    "history = model_class_0.fit(X_train, y_train_binary, epochs=100, batch_size=200,  # Increased epochs, reduced batch size\n",
    "                    validation_data=(X_valid, y_valid_binary),\n",
    "                    class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "773ed234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Classification Report:\n",
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "                   Other Classes       0.28      0.32      0.30        28\n",
      "Breast Invasive Ductal Carcinoma       0.84      0.81      0.82       120\n",
      "\n",
      "                        accuracy                           0.72       148\n",
      "                       macro avg       0.56      0.56      0.56       148\n",
      "                    weighted avg       0.73      0.72      0.72       148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' and 'X_valid' are already defined and model is trained\n",
    "\n",
    "# Predict the probabilities for the validation set\n",
    "y_pred_probs = model_class_0.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_valid_binary, y_pred_binary, target_names=['Other Classes', 'Breast Invasive Ductal Carcinoma'])\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4aa1591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Husse\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.5220 - loss: 3.0588 - val_accuracy: 0.3986 - val_loss: 0.8355\n",
      "Epoch 2/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5910 - loss: 1.5580 - val_accuracy: 0.4392 - val_loss: 0.8314\n",
      "Epoch 3/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6280 - loss: 1.2589 - val_accuracy: 0.2838 - val_loss: 1.0081\n",
      "Epoch 4/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6305 - loss: 1.1355 - val_accuracy: 0.6892 - val_loss: 0.5470\n",
      "Epoch 5/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6451 - loss: 1.1657 - val_accuracy: 0.8784 - val_loss: 0.3003\n",
      "Epoch 6/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7642 - loss: 0.9538 - val_accuracy: 0.7973 - val_loss: 0.4071\n",
      "Epoch 7/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7975 - loss: 0.8959 - val_accuracy: 0.6622 - val_loss: 0.6045\n",
      "Epoch 8/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7701 - loss: 0.6853 - val_accuracy: 0.8176 - val_loss: 0.3934\n",
      "Epoch 9/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8268 - loss: 0.6683 - val_accuracy: 0.7500 - val_loss: 0.5276\n",
      "Epoch 10/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8252 - loss: 0.6213 - val_accuracy: 0.8176 - val_loss: 0.4432\n",
      "Epoch 11/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8563 - loss: 0.4998 - val_accuracy: 0.7500 - val_loss: 0.5878\n",
      "Epoch 12/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8315 - loss: 0.6206 - val_accuracy: 0.7297 - val_loss: 0.6375\n",
      "Epoch 13/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8450 - loss: 0.4244 - val_accuracy: 0.8446 - val_loss: 0.3826\n",
      "Epoch 14/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9318 - loss: 0.2743 - val_accuracy: 0.8311 - val_loss: 0.4408\n",
      "Epoch 15/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9255 - loss: 0.2355 - val_accuracy: 0.7432 - val_loss: 0.6932\n",
      "Epoch 16/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9142 - loss: 0.2342 - val_accuracy: 0.8649 - val_loss: 0.3944\n",
      "Epoch 17/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9500 - loss: 0.1671 - val_accuracy: 0.8649 - val_loss: 0.3983\n",
      "Epoch 18/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.1450 - val_accuracy: 0.8784 - val_loss: 0.3826\n",
      "Epoch 19/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9808 - loss: 0.0751 - val_accuracy: 0.8851 - val_loss: 0.4336\n",
      "Epoch 20/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0842 - val_accuracy: 0.8581 - val_loss: 0.4980\n",
      "Epoch 21/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9911 - loss: 0.0464 - val_accuracy: 0.8649 - val_loss: 0.5271\n",
      "Epoch 22/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.1004 - val_accuracy: 0.8716 - val_loss: 0.5469\n",
      "Epoch 23/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9898 - loss: 0.0354 - val_accuracy: 0.8851 - val_loss: 0.5768\n",
      "Epoch 24/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9942 - loss: 0.0541 - val_accuracy: 0.8784 - val_loss: 0.5818\n",
      "Epoch 25/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9663 - loss: 0.1144 - val_accuracy: 0.8919 - val_loss: 0.5988\n",
      "Epoch 26/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9920 - loss: 0.0725 - val_accuracy: 0.8716 - val_loss: 0.6096\n",
      "Epoch 27/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9966 - loss: 0.0372 - val_accuracy: 0.8716 - val_loss: 0.5500\n",
      "Epoch 28/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9944 - loss: 0.0442 - val_accuracy: 0.8784 - val_loss: 0.5304\n",
      "Epoch 29/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9819 - loss: 0.0673 - val_accuracy: 0.8784 - val_loss: 0.5906\n",
      "Epoch 30/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9899 - loss: 0.0574 - val_accuracy: 0.8649 - val_loss: 0.5672\n",
      "Epoch 31/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9811 - loss: 0.0415 - val_accuracy: 0.8784 - val_loss: 0.5903\n",
      "Epoch 32/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9894 - loss: 0.0353 - val_accuracy: 0.8784 - val_loss: 0.6096\n",
      "Epoch 33/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9990 - loss: 0.0169 - val_accuracy: 0.8716 - val_loss: 0.6270\n",
      "Epoch 34/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9895 - loss: 0.0275 - val_accuracy: 0.8784 - val_loss: 0.6703\n",
      "Epoch 35/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9981 - loss: 0.0156 - val_accuracy: 0.8784 - val_loss: 0.6971\n",
      "Epoch 36/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9918 - loss: 0.0364 - val_accuracy: 0.8716 - val_loss: 0.6736\n",
      "Epoch 37/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9948 - loss: 0.0156 - val_accuracy: 0.8716 - val_loss: 0.6743\n",
      "Epoch 38/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9913 - loss: 0.0235 - val_accuracy: 0.8919 - val_loss: 0.6897\n",
      "Epoch 39/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9934 - loss: 0.0405 - val_accuracy: 0.8716 - val_loss: 0.6501\n",
      "Epoch 40/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9956 - loss: 0.0117 - val_accuracy: 0.8851 - val_loss: 0.6890\n",
      "Epoch 41/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9912 - loss: 0.0843 - val_accuracy: 0.8851 - val_loss: 0.6238\n",
      "Epoch 42/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9983 - loss: 0.0516 - val_accuracy: 0.8378 - val_loss: 0.7336\n",
      "Epoch 43/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0364 - val_accuracy: 0.8784 - val_loss: 0.5770\n",
      "Epoch 44/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.8919 - val_loss: 0.6180\n",
      "Epoch 45/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.8784 - val_loss: 0.6306\n",
      "Epoch 46/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9958 - loss: 0.0181 - val_accuracy: 0.8851 - val_loss: 0.6357\n",
      "Epoch 47/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9990 - loss: 0.0093 - val_accuracy: 0.8784 - val_loss: 0.6589\n",
      "Epoch 48/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9986 - loss: 0.0065 - val_accuracy: 0.8851 - val_loss: 0.6795\n",
      "Epoch 49/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.8851 - val_loss: 0.6805\n",
      "Epoch 50/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9992 - loss: 0.0059 - val_accuracy: 0.8919 - val_loss: 0.6757\n",
      "Epoch 51/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9976 - loss: 0.0068 - val_accuracy: 0.8919 - val_loss: 0.6739\n",
      "Epoch 52/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0034 - val_accuracy: 0.8919 - val_loss: 0.6901\n",
      "Epoch 53/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.8851 - val_loss: 0.6916\n",
      "Epoch 54/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.8919 - val_loss: 0.6992\n",
      "Epoch 55/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.8919 - val_loss: 0.7302\n",
      "Epoch 56/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.9767e-04 - val_accuracy: 0.8919 - val_loss: 0.7994\n",
      "Epoch 57/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.8986 - val_loss: 0.7572\n",
      "Epoch 58/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9994 - loss: 0.0051 - val_accuracy: 0.8784 - val_loss: 0.7784\n",
      "Epoch 59/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.8784 - val_loss: 0.7975\n",
      "Epoch 60/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.8851 - val_loss: 0.8201\n",
      "Epoch 61/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.8658e-04 - val_accuracy: 0.8851 - val_loss: 0.8283\n",
      "Epoch 62/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.8851 - val_loss: 0.8521\n",
      "Epoch 63/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 9.0341e-04 - val_accuracy: 0.8919 - val_loss: 0.8560\n",
      "Epoch 64/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.3344e-04 - val_accuracy: 0.8851 - val_loss: 0.8638\n",
      "Epoch 65/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.8533e-04 - val_accuracy: 0.8851 - val_loss: 0.8617\n",
      "Epoch 66/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.3208e-04 - val_accuracy: 0.8851 - val_loss: 0.8577\n",
      "Epoch 67/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.8851 - val_loss: 0.8670\n",
      "Epoch 68/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9991 - loss: 0.0023 - val_accuracy: 0.8851 - val_loss: 0.8404\n",
      "Epoch 69/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.1643e-04 - val_accuracy: 0.8986 - val_loss: 0.9381\n",
      "Epoch 70/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.8851 - val_loss: 0.8994\n",
      "Epoch 71/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.5292e-04 - val_accuracy: 0.8851 - val_loss: 0.8986\n",
      "Epoch 72/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.4559e-04 - val_accuracy: 0.8851 - val_loss: 0.9072\n",
      "Epoch 73/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 8.6490e-04 - val_accuracy: 0.8851 - val_loss: 0.9170\n",
      "Epoch 74/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.7417e-04 - val_accuracy: 0.8851 - val_loss: 0.9279\n",
      "Epoch 75/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9984 - loss: 0.0035 - val_accuracy: 0.8784 - val_loss: 0.8570\n",
      "Epoch 76/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9800 - loss: 0.1190 - val_accuracy: 0.8716 - val_loss: 0.9544\n",
      "Epoch 77/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9541 - loss: 0.2534 - val_accuracy: 0.8041 - val_loss: 0.9291\n",
      "Epoch 78/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9722 - loss: 0.0799 - val_accuracy: 0.8378 - val_loss: 0.5902\n",
      "Epoch 79/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9763 - loss: 0.0520 - val_accuracy: 0.8919 - val_loss: 0.6558\n",
      "Epoch 80/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9939 - loss: 0.0423 - val_accuracy: 0.8784 - val_loss: 0.6590\n",
      "Epoch 81/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9958 - loss: 0.0142 - val_accuracy: 0.8919 - val_loss: 0.6612\n",
      "Epoch 82/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0059 - val_accuracy: 0.8716 - val_loss: 0.6849\n",
      "Epoch 83/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.8919 - val_loss: 0.6721\n",
      "Epoch 84/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0055 - val_accuracy: 0.8919 - val_loss: 0.7171\n",
      "Epoch 85/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0036 - val_accuracy: 0.8919 - val_loss: 0.7631\n",
      "Epoch 86/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0042 - val_accuracy: 0.8919 - val_loss: 0.7203\n",
      "Epoch 87/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 9.1273e-04 - val_accuracy: 0.8919 - val_loss: 0.7260\n",
      "Epoch 88/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.8919 - val_loss: 0.7503\n",
      "Epoch 89/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.8919 - val_loss: 0.7560\n",
      "Epoch 90/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.8919 - val_loss: 0.7474\n",
      "Epoch 91/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9986 - loss: 0.0099 - val_accuracy: 0.8919 - val_loss: 0.7676\n",
      "Epoch 92/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.8919 - val_loss: 0.7746\n",
      "Epoch 93/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.8919 - val_loss: 0.7785\n",
      "Epoch 94/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.8919 - val_loss: 0.7842\n",
      "Epoch 95/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.8851 - val_loss: 0.7604\n",
      "Epoch 96/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 0.8784 - val_loss: 0.7776\n",
      "Epoch 97/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.8919 - val_loss: 0.8653\n",
      "Epoch 98/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0093 - val_accuracy: 0.8784 - val_loss: 0.7798\n",
      "Epoch 99/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.8784 - val_loss: 0.7875\n",
      "Epoch 100/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.8784 - val_loss: 0.7844\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_train and y_valid contain class labels\n",
    "# Let's say 'Class 1' is the new focus, adapt y_train_binary and y_valid_binary accordingly\n",
    "y_train_binary = (y_resampled == 1).astype(int)  # 1 for 'Class 1', 0 for others\n",
    "y_valid_binary = (y_valid == 1).astype(int)\n",
    "\n",
    "# Update class weights if needed\n",
    "class_weight_dict = {\n",
    "    0: 1,       # Normal weight for 'Other Classes'\n",
    "    1: 6       # Increased weight for 'Class 1'\n",
    "}\n",
    "input_shape = X_train.shape[1]  # Number of input features\n",
    "\n",
    "# Construct a new model architecture\n",
    "model_class_1 = Sequential([\n",
    "    Dense(250, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(156, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the optimizer with the desired learning rate\n",
    "adam_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with the customized optimizer\n",
    "model_class_1.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the new model\n",
    "history_class_1 = model_class_1.fit(X_resampled, y_train_binary, epochs=100, batch_size=50,\n",
    "                                    validation_data=(X_valid, y_valid_binary),\n",
    "                                    class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d43c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Classification Report for Class 1:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Other Classes       0.93      0.93      0.93       137\n",
      "      Class 1       0.18      0.18      0.18        11\n",
      "\n",
      "     accuracy                           0.88       148\n",
      "    macro avg       0.56      0.56      0.56       148\n",
      " weighted avg       0.88      0.88      0.88       148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities for the validation set\n",
    "y_pred_probs_class_1 = model_class_1.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary_class_1 = (y_pred_probs_class_1 > 0.5).astype(int)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report_class_1 = classification_report(y_valid_binary, y_pred_binary_class_1, \n",
    "                                       target_names=['Other Classes', 'Class 1'])\n",
    "print(\"Classification Report for Class 1:\\n\", report_class_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe1f0748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Husse\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4869 - loss: 0.7313 - val_accuracy: 0.3919 - val_loss: 0.9218\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7143 - loss: 0.5854 - val_accuracy: 0.3851 - val_loss: 0.9580\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7292 - loss: 0.5355 - val_accuracy: 0.5676 - val_loss: 0.7610\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8204 - loss: 0.4348 - val_accuracy: 0.6149 - val_loss: 0.6508\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8951 - loss: 0.3541 - val_accuracy: 0.7365 - val_loss: 0.5123\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9325 - loss: 0.2699 - val_accuracy: 0.6689 - val_loss: 0.6624\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9558 - loss: 0.1750 - val_accuracy: 0.7365 - val_loss: 0.5302\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9858 - loss: 0.1134 - val_accuracy: 0.7635 - val_loss: 0.5228\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0871 - val_accuracy: 0.7230 - val_loss: 0.6237\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0679 - val_accuracy: 0.7297 - val_loss: 0.6973\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9952 - loss: 0.0386 - val_accuracy: 0.7635 - val_loss: 0.6105\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0336 - val_accuracy: 0.7432 - val_loss: 0.7778\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9873 - loss: 0.0514 - val_accuracy: 0.8108 - val_loss: 0.5776\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9930 - loss: 0.0264 - val_accuracy: 0.8041 - val_loss: 0.6488\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9959 - loss: 0.0262 - val_accuracy: 0.7973 - val_loss: 0.6540\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9951 - loss: 0.0207 - val_accuracy: 0.7838 - val_loss: 0.7603\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9981 - loss: 0.0157 - val_accuracy: 0.7973 - val_loss: 0.7759\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0134 - val_accuracy: 0.8108 - val_loss: 0.7682\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0107 - val_accuracy: 0.8041 - val_loss: 0.7830\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0094 - val_accuracy: 0.7973 - val_loss: 0.8068\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0122 - val_accuracy: 0.7973 - val_loss: 0.8305\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9993 - loss: 0.0083 - val_accuracy: 0.7703 - val_loss: 0.9155\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9984 - loss: 0.0144 - val_accuracy: 0.7973 - val_loss: 0.8319\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0107 - val_accuracy: 0.7905 - val_loss: 0.9698\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0075 - val_accuracy: 0.8108 - val_loss: 0.9726\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train and y_valid contain class labels\n",
    "\n",
    "# Binarize y_train and y_valid so that 'Class 3' is 1 and all other classes are 0\n",
    "y_train_binary = (y_train == 3).astype(int)\n",
    "y_valid_binary = (y_valid == 3).astype(int)\n",
    "\n",
    "# Balancing the dataset with a combination of SMOTE and RandomUnderSampler\n",
    "smote = SMOTE(sampling_strategy={1: int(len(y_train_binary[y_train_binary == 0]) * 0.5)})  # Increase minority to 50% of majority\n",
    "under = RandomUnderSampler(sampling_strategy={0: len(y_train_binary[y_train_binary == 1]) * 2})  # Reduce majority to double of minority\n",
    "pipeline = Pipeline([('smote', smote), ('under', under)])\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train_binary)\n",
    "\n",
    "# Recalculate class weights after resampling\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_resampled), y=y_resampled)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Neural network model\n",
    "model_class_3 = Sequential([\n",
    "    Dense(128, input_dim=X_resampled.shape[1]),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32),\n",
    "    Activation('relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model_class_3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = model_class_3.fit(\n",
    "    X_resampled, y_resampled,\n",
    "    epochs=100,\n",
    "    batch_size=50,\n",
    "    validation_data=(X_valid, y_valid_binary),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d5f0682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Validation ROC AUC: 0.67\n",
      "Classification Report for Class 3:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Other Classes       0.90      0.79      0.84       131\n",
      "      Class 3       0.16      0.29      0.20        17\n",
      "\n",
      "     accuracy                           0.74       148\n",
      "    macro avg       0.53      0.54      0.52       148\n",
      " weighted avg       0.81      0.74      0.77       148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "val_predictions = model_class_3.predict(X_valid)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_valid_binary, val_predictions)\n",
    "print(f\"Validation ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Generate and print classification report\n",
    "report = classification_report(y_valid_binary, val_predictions_binary, target_names=['Other Classes', 'Class 3'])\n",
    "print(\"Classification Report for Class 3:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc247b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Confusion Matrix:\n",
      "[[100   3   9   8]\n",
      " [  8   1   1   2]\n",
      " [ 16   0   0   2]\n",
      " [ 10   2   2   2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.75      0.83      0.79       120\n",
      "     Class 1       0.17      0.08      0.11        12\n",
      "     Class 2       0.00      0.00      0.00        18\n",
      "     Class 3       0.14      0.12      0.13        16\n",
      "\n",
      "    accuracy                           0.62       166\n",
      "   macro avg       0.26      0.26      0.26       166\n",
      "weighted avg       0.57      0.62      0.59       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predefined model accuracies\n",
    "model_accuracies = {\n",
    "    'model_0': 0.90,  # Example accuracy for model_0\n",
    "    'model_1': 0.85,  # Example accuracy for model_1\n",
    "    'model_3': 0.88   # Example accuracy for model_3\n",
    "}\n",
    "\n",
    "def predict_with_customization(X_test, y_test, model_0, model_1, model_3):\n",
    "    predictions = []\n",
    "\n",
    "    X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    X_test = tf.reshape(X_test, [-1, 734])  # Adjust to the correct feature size\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        try:\n",
    "            example = X_test[i]\n",
    "            example = tf.expand_dims(example, axis=0)\n",
    "\n",
    "            # Predict using all three models\n",
    "            pred_0 = model_0.predict(example)[0, 0]\n",
    "            pred_1 = model_1.predict(example)[0, 0]\n",
    "            pred_3 = model_3.predict(example)[0, 0]\n",
    "\n",
    "            # Collect predictions based on the threshold\n",
    "            votes = [(0, pred_0 >= 0.5), (1, pred_1 >= 0.5), (3, pred_3 >= 0.5)]\n",
    "            active_votes = [v[0] for v in votes if v[1]]\n",
    "\n",
    "            # Decide the final prediction based on majority or the highest accuracy in the case of a tie\n",
    "            if len(active_votes) == 0:\n",
    "                predictions.append(2)  # Default to class 2 if no model is confident\n",
    "            elif len(set(active_votes)) == 1:\n",
    "                predictions.append(active_votes[0])  # Only one class predicted\n",
    "            else:\n",
    "                # In case of a tie, use the model with the highest accuracy among the predicted classes\n",
    "                predicted_class = max(active_votes, key=lambda x: model_accuracies[f'model_{x}'])\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {e}\")\n",
    "            predictions.append(2)  # Fallback to class 2 on error\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    report = classification_report(y_test, predictions, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Visualizing the confusion matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],\n",
    "                yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "\n",
    "\n",
    "predict_with_customization(X_test, y_test, model_class_0, model_class_1, model_class_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a59e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04719eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
