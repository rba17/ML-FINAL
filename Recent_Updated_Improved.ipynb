{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0517f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ryana\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryana\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ryana\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ryana\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ryana\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: keras in c:\\users\\ryana\\anaconda3\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\ryana\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d65c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,cross_validate,StratifiedKFold\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report,roc_auc_score\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6a7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryana\\AppData\\Local\\Temp\\ipykernel_17248\\4014827458.py:1: DtypeWarning: Columns (662,664,676,677,683,685,686,687) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cancer_type</th>\n",
       "      <th>age_at_diagnosis</th>\n",
       "      <th>cellularity</th>\n",
       "      <th>chemotherapy</th>\n",
       "      <th>pam50_+_claudin-low_subtype</th>\n",
       "      <th>cohort</th>\n",
       "      <th>er_status</th>\n",
       "      <th>neoplasm_histologic_grade</th>\n",
       "      <th>her2_status_measured_by_snp6</th>\n",
       "      <th>her2_status</th>\n",
       "      <th>...</th>\n",
       "      <th>mtap_mut</th>\n",
       "      <th>ppp2cb_mut</th>\n",
       "      <th>smarcd1_mut</th>\n",
       "      <th>nras_mut</th>\n",
       "      <th>ndfip1_mut</th>\n",
       "      <th>hras_mut</th>\n",
       "      <th>prps2_mut</th>\n",
       "      <th>smarcb1_mut</th>\n",
       "      <th>stmn2_mut</th>\n",
       "      <th>siah1_mut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>54.29</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>LumB</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>43.45</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0</td>\n",
       "      <td>LumA</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>74.11</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>LumB</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>51.87</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>LumA</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>87.18</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0</td>\n",
       "      <td>LumB</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GAIN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 686 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cancer_type  age_at_diagnosis cellularity  \\\n",
       "0  Breast Invasive Ductal Carcinoma             54.29        High   \n",
       "1  Breast Invasive Ductal Carcinoma             43.45    Moderate   \n",
       "2  Breast Invasive Ductal Carcinoma             74.11        High   \n",
       "3  Breast Invasive Ductal Carcinoma             51.87        High   \n",
       "4  Breast Invasive Ductal Carcinoma             87.18    Moderate   \n",
       "\n",
       "   chemotherapy pam50_+_claudin-low_subtype  cohort er_status  \\\n",
       "0             1                        LumB       1  Positive   \n",
       "1             0                        LumA       4  Positive   \n",
       "2             0                        LumB       3  Positive   \n",
       "3             0                        LumA       3  Positive   \n",
       "4             0                        LumB       1  Positive   \n",
       "\n",
       "   neoplasm_histologic_grade her2_status_measured_by_snp6 her2_status  ...  \\\n",
       "0                        3.0                      NEUTRAL    Negative  ...   \n",
       "1                        1.0                         LOSS    Negative  ...   \n",
       "2                        3.0                      NEUTRAL    Negative  ...   \n",
       "3                        2.0                      NEUTRAL    Negative  ...   \n",
       "4                        3.0                         GAIN    Positive  ...   \n",
       "\n",
       "   mtap_mut ppp2cb_mut smarcd1_mut  nras_mut  ndfip1_mut  hras_mut  prps2_mut  \\\n",
       "0         0          0           0         0           0         0          0   \n",
       "1         0          0           0         0           0         0          0   \n",
       "2         0          0           0         0           0         0          0   \n",
       "3         0          0           0         0           0         0          0   \n",
       "4         0          0           0         0           0         0          0   \n",
       "\n",
       "   smarcb1_mut stmn2_mut  siah1_mut  \n",
       "0            0         0          0  \n",
       "1            0         0          0  \n",
       "2            0         0          0  \n",
       "3            0         0          0  \n",
       "4            0         0          0  \n",
       "\n",
       "[5 rows x 686 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data_cleaned = data.drop(columns=['patient_id','er_status_measured_by_ihc'])\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b877187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_with_mode(data):\n",
    "    # Get a list of columns with missing values\n",
    "    columns_with_missing_values = data.columns[data.isnull().any()].tolist()\n",
    "    \n",
    "    # Iterate over each column with missing values\n",
    "    for column_name in columns_with_missing_values:\n",
    "        # Loop until no more null values in the column\n",
    "        while data[column_name].isnull().any():\n",
    "            # Iterate over rows in the DataFrame\n",
    "            for i, row in data[data[column_name].isnull()].iterrows():\n",
    "                # Filter the data for the same cancer type\n",
    "                same_type_data = data[data['cancer_type'] == row['cancer_type']]\n",
    "                \n",
    "                # Try to find 5 other entries; if fewer, take as many as available\n",
    "                if len(same_type_data) > 5:\n",
    "                    sample = same_type_data.sample(n=5)\n",
    "                else:\n",
    "                    sample = same_type_data\n",
    "                \n",
    "                # Calculate the mode of the selected sample\n",
    "                mode_value = sample[column_name].mode()\n",
    "                \n",
    "                # If mode calculation is successful and not empty, use the mode to fill the missing value\n",
    "                if not mode_value.empty:\n",
    "                    data.at[i, column_name] = mode_value.iloc[0]\n",
    "                else:\n",
    "                    # If no mode available (all values are different or no other samples), we might choose to do nothing or use a global mode\n",
    "                    # Here we're choosing to use the global mode as a fallback\n",
    "                    global_mode = data[column_name].mode()[0]\n",
    "                    data.at[i, column_name] = global_mode\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage for the column 'cellularity'\n",
    "updated_data = fill_missing_with_mode(data_cleaned)\n",
    "\n",
    "missing_values = updated_data.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Display columns with missing values and their counts\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2407a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in X after encoding: 734\n"
     ]
    }
   ],
   "source": [
    "X = data_cleaned.drop(columns=['cancer_type'])\n",
    "y = data_cleaned['cancer_type']\n",
    "\n",
    "cols_one_hot = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() <= 5]\n",
    "cols_label = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() > 5]\n",
    "\n",
    "# Initialize the transformers list for the ColumnTransformer\n",
    "transformers = []\n",
    "\n",
    "# Loop through each column in X to apply appropriate encoding\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object' or X[column].dtype == 'int':  # Adjusted to ensure we catch int types too\n",
    "        X[column] = X[column].astype(str)  # Convert everything to string to avoid mixed type errors\n",
    "        unique_values = X[column].nunique()\n",
    "        if unique_values > 5:\n",
    "            # Use LabelEncoder for columns with more than 5 unique values\n",
    "            transformers.append((column, LabelEncoder(), [column]))  # LabelEncoder usage adjusted\n",
    "        else:\n",
    "            # Use OneHotEncoder for columns with 5 or fewer unique values\n",
    "            transformers.append((column, OneHotEncoder(), [column]))\n",
    "\n",
    "# Manually apply LabelEncoder to the relevant columns before ColumnTransformer\n",
    "for name, encoder, columns in transformers:\n",
    "    if isinstance(encoder, LabelEncoder):\n",
    "        X[columns[0]] = encoder.fit_transform(X[columns[0]])  # Directly encode the column in the DataFrame\n",
    "        # Remove label encoded columns from transformer list since they are already processed\n",
    "        transformers = [(n, e, c) for n, e, c in transformers if e is not LabelEncoder]\n",
    "\n",
    "# Setup remaining transformations with OneHotEncoder using ColumnTransformer\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), [name for name, encoder, _ in transformers if isinstance(encoder, OneHotEncoder)])\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep all other columns that do not need encoding\n",
    ")\n",
    "\n",
    "# Fit and transform X with the defined ColumnTransformer for OneHotEncoder\n",
    "X_transformed = preprocessor_X.fit_transform(X)\n",
    "\n",
    "# Label encode y\n",
    "label_encoder_y = LabelEncoder()\n",
    "y_encoded = label_encoder_y.fit_transform(y)\n",
    "\n",
    "# Print the final shape of X_transformed to verify feature count\n",
    "print(\"Number of features in X after encoding:\", X_transformed.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ef713b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names and counts are consistent.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ahnak2_mut</th>\n",
       "      <th>kmt2c_mut</th>\n",
       "      <th>syne1_mut</th>\n",
       "      <th>gata3_mut</th>\n",
       "      <th>map3k1_mut</th>\n",
       "      <th>ahnak_mut</th>\n",
       "      <th>dnah11_mut</th>\n",
       "      <th>cdh1_mut</th>\n",
       "      <th>dnah2_mut</th>\n",
       "      <th>kmt2d_mut</th>\n",
       "      <th>...</th>\n",
       "      <th>spry2</th>\n",
       "      <th>srd5a1</th>\n",
       "      <th>srd5a2</th>\n",
       "      <th>srd5a3</th>\n",
       "      <th>st7</th>\n",
       "      <th>star</th>\n",
       "      <th>tnk2</th>\n",
       "      <th>tulp4</th>\n",
       "      <th>ugt2b15</th>\n",
       "      <th>ugt2b17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.160</td>\n",
       "      <td>111.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.046</td>\n",
       "      <td>76.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.056</td>\n",
       "      <td>118.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.028</td>\n",
       "      <td>220.233333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.052</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 658 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ahnak2_mut  kmt2c_mut  syne1_mut  gata3_mut  map3k1_mut   ahnak_mut  \\\n",
       "0         3.0        1.0        3.0        2.0       5.160  111.100000   \n",
       "1         1.0        1.0        0.0        7.0       2.046   76.866667   \n",
       "2         3.0        1.0        6.0        3.0       6.056  118.700000   \n",
       "3         2.0        0.0        0.0       10.0       3.028  220.233333   \n",
       "4         3.0        1.0        2.0        1.0       5.052   28.600000   \n",
       "\n",
       "   dnah11_mut  cdh1_mut  dnah2_mut  kmt2d_mut  ...  spry2  srd5a1  srd5a2  \\\n",
       "0         0.0       1.0       80.0        3.0  ...    0.0     0.0     0.0   \n",
       "1         1.0       1.0       23.0        2.0  ...    0.0     0.0     0.0   \n",
       "2         0.0       1.0       28.0        2.0  ...    0.0     0.0     0.0   \n",
       "3         1.0       1.0       14.0        1.0  ...    0.0     0.0     0.0   \n",
       "4         0.0       0.0       26.0        2.0  ...    0.0     0.0     0.0   \n",
       "\n",
       "   srd5a3  st7  star  tnk2  tulp4  ugt2b15  ugt2b17  \n",
       "0     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "1     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "2     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "3     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "4     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 658 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Assume cols_label and cols_one_hot are defined and scoped correctly from a previous cell\n",
    "onehot_features = preprocessor_X.named_transformers_['onehot'].get_feature_names_out()\n",
    "label_features = cols_label  # Using the columns designated for label encoding\n",
    "remainder_features = [col for col in X.columns if col not in cols_one_hot and col not in cols_label]\n",
    "\n",
    "# Combining all feature names\n",
    "all_features = list(onehot_features) + label_features + remainder_features\n",
    "\n",
    "# Create the DataFrame from the transformed data\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns=all_features[:X_transformed.shape[1]])\n",
    "\n",
    "# Verify column counts and consistency\n",
    "if len(all_features[:X_transformed.shape[1]]) != X_transformed_df.shape[1]:\n",
    "    print(f\"Warning: Column count mismatch. {len(all_features[:X_transformed.shape[1]])} names for {X_transformed_df.shape[1]} actual columns.\")\n",
    "else:\n",
    "    print(\"Column names and counts are consistent.\")\n",
    "\n",
    "# Find the index of 'muc16_mut' column\n",
    "muc16_mut_index = X_transformed_df.columns.get_loc(\"muc16_mut\")\n",
    "\n",
    "# Select columns after 'muc16_mut'\n",
    "selected_columns = X_transformed_df.columns[muc16_mut_index + 1:]\n",
    "\n",
    "# Display the selected columns for the first 30 rows of the DataFrame\n",
    "display(X_transformed_df[selected_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e2b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      brca1     brca2     palb2      pten      tp53       atm      cdh1  \\\n",
      "0 -1.211576 -0.109631 -0.594727 -1.140829  0.595648 -1.106277 -2.143642   \n",
      "1  0.427194 -0.010864 -0.736981  0.064916  0.152814  0.097883  0.103718   \n",
      "2 -0.477025 -0.151244 -0.772095 -0.847137  0.240765 -0.248629 -0.571054   \n",
      "3 -0.548368  0.201905 -0.655579 -0.891006 -0.371917  1.477406  0.814926   \n",
      "4  0.972271  0.990202 -0.241689  0.815784  0.212176  0.843807 -0.182637   \n",
      "\n",
      "      chek2       nbn       nf1  ...     spry2    srd5a1    srd5a2    srd5a3  \\\n",
      "0 -1.105545 -0.703869 -0.394923  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "1 -0.322918  0.518080 -0.696819  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "2  0.063015  0.116458  0.202719  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "3  0.903104  0.189407 -0.800239  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "4 -0.134838 -0.517778  1.051354  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "\n",
      "        st7      star      tnk2     tulp4   ugt2b15   ugt2b17  \n",
      "0 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "1 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "2 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "3 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "4 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "\n",
      "[5 rows x 488 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_index = X_transformed_df.columns.get_loc(\"brca1\")\n",
    "end_index = X_transformed_df.columns.get_loc(\"ugt2b17\") +1\n",
    "\n",
    "# Select the columns for scaling\n",
    "columns_to_scale = X_transformed_df.columns[start_index:end_index]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the selected columns\n",
    "X_transformed_df[columns_to_scale] = scaler.fit_transform(X_transformed_df[columns_to_scale])\n",
    "\n",
    "# Display the scaled data for these columns\n",
    "print(X_transformed_df[columns_to_scale].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eee224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (1196, 734) (1196,)\n",
      "Validation set size: (150, 734) (150,)\n",
      "Test set size: (150, 734) (150,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "    X_transformed_df, y_encoded, train_size=0.8, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Second split on the remaining 20% to get 10% Validation and 10% Test\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_remaining, y_remaining, test_size=0.5, random_state=42, stratify=y_remaining\n",
    ")\n",
    "\n",
    "# Print the sizes of each dataset to confirm the splits\n",
    "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set size:\", X_valid.shape, y_valid.shape)\n",
    "print(\"Test set size:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c15ec4fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Invasive Ductal Carcinoma             959\n",
      "Breast Mixed Ductal and Lobular Carcinoma    132\n",
      "Breast Invasive Lobular Carcinoma             91\n",
      "Breast Invasive Mixed Mucinous Carcinoma      14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train to a pandas Series for easy counting\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "# Get counts of each class\n",
    "class_counts_train = y_train_series.value_counts()\n",
    "\n",
    "# Print the counts with original class names using inverse transform of LabelEncoder\n",
    "class_names_counts_train = pd.Series(class_counts_train.index).apply(\n",
    "    lambda x: label_encoder_y.inverse_transform([x])[0])\n",
    "class_counts_train.index = class_names_counts_train\n",
    "\n",
    "# Display the class counts with names\n",
    "print(class_counts_train)\n",
    "\n",
    "# Class Number 0: Breast Invasive Ductal Carcinoma\n",
    "# Class Number 1: Breast Invasive Lobular Carcinoma\n",
    "# Class Number 2: Breast Invasive Mixed Mucinous Carcinoma\n",
    "# Class Number 3: Breast Mixed Ductal and Lobular Carcinoma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1804559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a180d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(900, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Instantiate the optimizer with the desired learning rate decay\n",
    "initial_lr = 0.001\n",
    "adam_optimizer = Adam(learning_rate=initial_lr)\n",
    "\n",
    "# Compile the model with the customized optimizer\n",
    "model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c136425f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.2507 - loss: 1.9525 - val_accuracy: 0.2133 - val_loss: 2.2807 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.2800 - loss: 1.4149 - val_accuracy: 0.2000 - val_loss: 2.4201 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.3031 - loss: 1.1301 - val_accuracy: 0.1800 - val_loss: 3.0629 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.3059 - loss: 1.2353 - val_accuracy: 0.1667 - val_loss: 4.0184 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.3491 - loss: 1.0473 - val_accuracy: 0.2200 - val_loss: 3.2347 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy: 0.3503 - loss: 0.9698 - val_accuracy: 0.3067 - val_loss: 2.1046 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - accuracy: 0.3941 - loss: 0.9470 - val_accuracy: 0.3400 - val_loss: 1.6252 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 0.3876 - loss: 0.8166 - val_accuracy: 0.3200 - val_loss: 1.5928 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.4044 - loss: 0.8162 - val_accuracy: 0.2800 - val_loss: 1.6909 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.4190 - loss: 0.7450 - val_accuracy: 0.2667 - val_loss: 1.8424 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.4452 - loss: 0.7242 - val_accuracy: 0.2067 - val_loss: 2.1199 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.4510 - loss: 0.6790 - val_accuracy: 0.2200 - val_loss: 1.9111 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 0.4830 - loss: 0.6416 - val_accuracy: 0.2800 - val_loss: 1.6315 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.4652 - loss: 0.6106 - val_accuracy: 0.2733 - val_loss: 1.5875 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.4721 - loss: 0.5755 - val_accuracy: 0.2800 - val_loss: 1.5459 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.4831 - loss: 0.5454 - val_accuracy: 0.3333 - val_loss: 1.4454 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 0.5053 - loss: 0.5264 - val_accuracy: 0.3267 - val_loss: 1.3619 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.5177 - loss: 0.5345 - val_accuracy: 0.3733 - val_loss: 1.3643 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - accuracy: 0.5334 - loss: 0.4853 - val_accuracy: 0.3533 - val_loss: 1.4153 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.5422 - loss: 0.4969 - val_accuracy: 0.3600 - val_loss: 1.4106 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.5324 - loss: 0.4808 - val_accuracy: 0.3800 - val_loss: 1.3744 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.5592 - loss: 0.4512 - val_accuracy: 0.3933 - val_loss: 1.3152 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.5936 - loss: 0.4270 - val_accuracy: 0.4267 - val_loss: 1.2399 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.5870 - loss: 0.4459 - val_accuracy: 0.4733 - val_loss: 1.1936 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 0.5687 - loss: 0.4005 - val_accuracy: 0.4733 - val_loss: 1.1422 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 0.5798 - loss: 0.3945 - val_accuracy: 0.4867 - val_loss: 1.1107 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.6101 - loss: 0.4012 - val_accuracy: 0.5000 - val_loss: 1.0961 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.6343 - loss: 0.3713 - val_accuracy: 0.5400 - val_loss: 1.0748 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.6189 - loss: 0.3481 - val_accuracy: 0.5600 - val_loss: 1.0530 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.6344 - loss: 0.3463 - val_accuracy: 0.5933 - val_loss: 1.0105 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.6679 - loss: 0.3273 - val_accuracy: 0.6400 - val_loss: 0.9718 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - accuracy: 0.6896 - loss: 0.3219 - val_accuracy: 0.6600 - val_loss: 0.9291 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.6948 - loss: 0.3108 - val_accuracy: 0.6867 - val_loss: 0.8819 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.7115 - loss: 0.3035 - val_accuracy: 0.7067 - val_loss: 0.8526 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 0.7270 - loss: 0.2855 - val_accuracy: 0.7133 - val_loss: 0.8336 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.7075 - loss: 0.2797 - val_accuracy: 0.7200 - val_loss: 0.8220 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.7415 - loss: 0.2651 - val_accuracy: 0.7267 - val_loss: 0.8121 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.7593 - loss: 0.2661 - val_accuracy: 0.7400 - val_loss: 0.7932 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.7636 - loss: 0.2476 - val_accuracy: 0.7467 - val_loss: 0.7765 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.7615 - loss: 0.2459 - val_accuracy: 0.7467 - val_loss: 0.7630 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - accuracy: 0.7940 - loss: 0.2261 - val_accuracy: 0.7600 - val_loss: 0.7549 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - accuracy: 0.7874 - loss: 0.2300 - val_accuracy: 0.7467 - val_loss: 0.7513 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.8139 - loss: 0.2177 - val_accuracy: 0.7333 - val_loss: 0.7490 - learning_rate: 5.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.8152 - loss: 0.2161 - val_accuracy: 0.7467 - val_loss: 0.7425 - learning_rate: 5.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.8261 - loss: 0.2461 - val_accuracy: 0.7400 - val_loss: 0.7421 - learning_rate: 5.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.8391 - loss: 0.1910 - val_accuracy: 0.7400 - val_loss: 0.7406 - learning_rate: 5.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.8529 - loss: 0.1785 - val_accuracy: 0.7400 - val_loss: 0.7388 - learning_rate: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.8426 - loss: 0.1830 - val_accuracy: 0.7667 - val_loss: 0.7420 - learning_rate: 5.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.8674 - loss: 0.1734 - val_accuracy: 0.7933 - val_loss: 0.7464 - learning_rate: 5.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.8710 - loss: 0.1715 - val_accuracy: 0.7867 - val_loss: 0.7483 - learning_rate: 5.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.8832 - loss: 0.1622 - val_accuracy: 0.7733 - val_loss: 0.7412 - learning_rate: 5.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.8786 - loss: 0.1531 - val_accuracy: 0.7667 - val_loss: 0.7380 - learning_rate: 5.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.8917 - loss: 0.1524 - val_accuracy: 0.7600 - val_loss: 0.7433 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9011 - loss: 0.1348 - val_accuracy: 0.7600 - val_loss: 0.7522 - learning_rate: 5.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.9137 - loss: 0.1315 - val_accuracy: 0.7600 - val_loss: 0.7684 - learning_rate: 5.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.9022 - loss: 0.1329 - val_accuracy: 0.7600 - val_loss: 0.8033 - learning_rate: 5.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.9085 - loss: 0.1311 - val_accuracy: 0.7467 - val_loss: 0.8163 - learning_rate: 5.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.9198 - loss: 0.1210 - val_accuracy: 0.7600 - val_loss: 0.8106 - learning_rate: 2.5000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - accuracy: 0.9023 - loss: 0.1368 - val_accuracy: 0.7667 - val_loss: 0.8011 - learning_rate: 2.5000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.9217 - loss: 0.1076 - val_accuracy: 0.7667 - val_loss: 0.7977 - learning_rate: 2.5000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.9233 - loss: 0.1127 - val_accuracy: 0.7733 - val_loss: 0.7948 - learning_rate: 2.5000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.9354 - loss: 0.1081 - val_accuracy: 0.7733 - val_loss: 0.7957 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=512,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd0d61da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Summarize history for accuracy\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Summarize history for loss\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac1fdee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 302ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       120\n",
      "           1       0.22      0.18      0.20        11\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.25      0.06      0.10        17\n",
      "\n",
      "    accuracy                           0.77       150\n",
      "   macro avg       0.32      0.29      0.29       150\n",
      "weighted avg       0.70      0.77      0.72       150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_valid, y_pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bbfedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\ryana\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e23298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution: [500  91  14 132]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train contains the class labels, and X_train contains the features\n",
    "\n",
    "# Setup the RandomUnderSampler to reduce class 0 to 500 instances\n",
    "rus = RandomUnderSampler(sampling_strategy={0: 500})\n",
    "\n",
    "# Apply the resampling to the training data\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution to ensure it is as expected\n",
    "print(\"New class distribution:\", np.bincount(y_resampled))\n",
    "\n",
    "# Now X_resampled and y_resampled contain the undersampled data where class 0 has exactly 500 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0c700b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 303ms/step - accuracy: 0.3878 - loss: 1.9330 - val_accuracy: 0.2133 - val_loss: 1.0600\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.3328 - loss: 1.5373 - val_accuracy: 0.6667 - val_loss: 0.6274\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.4430 - loss: 1.4366 - val_accuracy: 0.5200 - val_loss: 0.7504\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5396 - loss: 1.2915 - val_accuracy: 0.5400 - val_loss: 0.7746\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.4799 - loss: 1.2751 - val_accuracy: 0.5867 - val_loss: 0.6737\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.5958 - loss: 1.2075 - val_accuracy: 0.5667 - val_loss: 0.6687\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5980 - loss: 1.1648 - val_accuracy: 0.5600 - val_loss: 0.7117\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.5973 - loss: 1.1227 - val_accuracy: 0.6267 - val_loss: 0.6781\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6528 - loss: 1.0293 - val_accuracy: 0.5667 - val_loss: 0.7272\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6113 - loss: 0.9866 - val_accuracy: 0.6867 - val_loss: 0.5740\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7125 - loss: 1.0210 - val_accuracy: 0.6133 - val_loss: 0.6899\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7606 - loss: 0.8124 - val_accuracy: 0.5667 - val_loss: 0.7464\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6818 - loss: 0.8206 - val_accuracy: 0.6600 - val_loss: 0.6252\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8140 - loss: 0.6756 - val_accuracy: 0.5800 - val_loss: 0.8280\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7967 - loss: 0.6548 - val_accuracy: 0.6733 - val_loss: 0.6358\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8116 - loss: 0.5956 - val_accuracy: 0.7067 - val_loss: 0.5393\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8785 - loss: 0.6011 - val_accuracy: 0.6333 - val_loss: 0.7819\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8473 - loss: 0.5162 - val_accuracy: 0.6933 - val_loss: 0.6753\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8540 - loss: 0.4436 - val_accuracy: 0.7133 - val_loss: 0.6253\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9013 - loss: 0.3484 - val_accuracy: 0.7133 - val_loss: 0.6286\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9243 - loss: 0.3252 - val_accuracy: 0.6867 - val_loss: 0.6689\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9566 - loss: 0.2359 - val_accuracy: 0.6667 - val_loss: 0.8451\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9222 - loss: 0.2441 - val_accuracy: 0.6867 - val_loss: 0.7786\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9461 - loss: 0.1923 - val_accuracy: 0.7000 - val_loss: 0.8306\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9477 - loss: 0.1637 - val_accuracy: 0.6933 - val_loss: 0.8280\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9512 - loss: 0.1515 - val_accuracy: 0.7467 - val_loss: 0.7423\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9761 - loss: 0.1397 - val_accuracy: 0.7067 - val_loss: 0.7632\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9730 - loss: 0.1517 - val_accuracy: 0.7067 - val_loss: 0.8802\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9643 - loss: 0.1384 - val_accuracy: 0.7200 - val_loss: 0.8909\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9843 - loss: 0.0730 - val_accuracy: 0.7333 - val_loss: 0.8717\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9765 - loss: 0.1114 - val_accuracy: 0.7467 - val_loss: 0.9288\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9812 - loss: 0.1295 - val_accuracy: 0.7000 - val_loss: 0.9985\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9907 - loss: 0.0674 - val_accuracy: 0.6933 - val_loss: 1.0409\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9836 - loss: 0.0714 - val_accuracy: 0.7800 - val_loss: 0.8914\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9888 - loss: 0.0730 - val_accuracy: 0.7000 - val_loss: 1.0501\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9841 - loss: 0.0651 - val_accuracy: 0.7467 - val_loss: 0.9409\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9938 - loss: 0.0595 - val_accuracy: 0.6867 - val_loss: 1.1406\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9955 - loss: 0.0425 - val_accuracy: 0.7133 - val_loss: 1.0604\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9983 - loss: 0.0153 - val_accuracy: 0.7200 - val_loss: 1.1232\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9960 - loss: 0.0305 - val_accuracy: 0.7067 - val_loss: 1.1503\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9866 - loss: 0.0449 - val_accuracy: 0.7267 - val_loss: 1.1826\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9998 - loss: 0.0220 - val_accuracy: 0.7333 - val_loss: 1.2047\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9957 - loss: 0.0345 - val_accuracy: 0.6600 - val_loss: 1.3803\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9865 - loss: 0.0299 - val_accuracy: 0.7800 - val_loss: 1.1161\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9924 - loss: 0.0611 - val_accuracy: 0.6800 - val_loss: 1.4225\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9979 - loss: 0.0243 - val_accuracy: 0.7200 - val_loss: 1.3169\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9951 - loss: 0.0164 - val_accuracy: 0.7067 - val_loss: 1.2460\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9962 - loss: 0.0253 - val_accuracy: 0.7133 - val_loss: 1.2642\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9954 - loss: 0.0276 - val_accuracy: 0.6933 - val_loss: 1.3505\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9958 - loss: 0.0173 - val_accuracy: 0.7333 - val_loss: 1.2227\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0178 - val_accuracy: 0.7133 - val_loss: 1.2305\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9981 - loss: 0.0135 - val_accuracy: 0.7133 - val_loss: 1.2530\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9988 - loss: 0.0152 - val_accuracy: 0.6800 - val_loss: 1.4072\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.7200 - val_loss: 1.3652\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9998 - loss: 0.0105 - val_accuracy: 0.7267 - val_loss: 1.3880\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9994 - loss: 0.0068 - val_accuracy: 0.7000 - val_loss: 1.4064\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9990 - loss: 0.0073 - val_accuracy: 0.7133 - val_loss: 1.4074\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9998 - loss: 0.0054 - val_accuracy: 0.7533 - val_loss: 1.3487\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9998 - loss: 0.0052 - val_accuracy: 0.7400 - val_loss: 1.4167\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9994 - loss: 0.0073 - val_accuracy: 0.7133 - val_loss: 1.4898\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0076 - val_accuracy: 0.7533 - val_loss: 1.3722\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0109 - val_accuracy: 0.7200 - val_loss: 1.4613\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9959 - loss: 0.0088 - val_accuracy: 0.7400 - val_loss: 1.3972\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9988 - loss: 0.0134 - val_accuracy: 0.7000 - val_loss: 1.4677\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0080 - val_accuracy: 0.7133 - val_loss: 1.4548\n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.7467 - val_loss: 1.4332\n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9998 - loss: 0.0061 - val_accuracy: 0.7400 - val_loss: 1.4537\n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9988 - loss: 0.0051 - val_accuracy: 0.6733 - val_loss: 1.5497\n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.7200 - val_loss: 1.4878\n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9988 - loss: 0.0073 - val_accuracy: 0.7400 - val_loss: 1.5005\n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9972 - loss: 0.0092 - val_accuracy: 0.7333 - val_loss: 1.5419\n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 0.7400 - val_loss: 1.5301\n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7400 - val_loss: 1.5378\n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7200 - val_loss: 1.6121\n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7000 - val_loss: 1.5802\n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9994 - loss: 0.0038 - val_accuracy: 0.7600 - val_loss: 1.4781\n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.7400 - val_loss: 1.5200\n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.6933 - val_loss: 1.6257\n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9994 - loss: 0.0058 - val_accuracy: 0.7267 - val_loss: 1.5311\n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9981 - loss: 0.0088 - val_accuracy: 0.7467 - val_loss: 1.5555\n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9994 - loss: 0.0083 - val_accuracy: 0.7467 - val_loss: 1.5117\n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7400 - val_loss: 1.4926\n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7533 - val_loss: 1.4827\n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.7333 - val_loss: 1.5039\n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9933 - loss: 0.0431 - val_accuracy: 0.6933 - val_loss: 1.8589\n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9963 - loss: 0.0084 - val_accuracy: 0.7533 - val_loss: 1.6859\n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7333 - val_loss: 1.7140\n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.7133 - val_loss: 1.7640\n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9980 - loss: 0.0054 - val_accuracy: 0.7400 - val_loss: 1.5511\n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.7400 - val_loss: 1.5043\n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9998 - loss: 0.0020 - val_accuracy: 0.7133 - val_loss: 1.6750\n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9963 - loss: 0.0076 - val_accuracy: 0.7133 - val_loss: 1.5167\n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9988 - loss: 0.0095 - val_accuracy: 0.7200 - val_loss: 1.6008\n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9994 - loss: 0.0040 - val_accuracy: 0.7267 - val_loss: 1.6058\n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9981 - loss: 0.0088 - val_accuracy: 0.7267 - val_loss: 1.5433\n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0058 - val_accuracy: 0.6667 - val_loss: 1.9451\n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9928 - loss: 0.0301 - val_accuracy: 0.7733 - val_loss: 1.5131\n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9964 - loss: 0.0132 - val_accuracy: 0.7333 - val_loss: 1.5725\n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.7000 - val_loss: 1.8584\n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9970 - loss: 0.0077 - val_accuracy: 0.7600 - val_loss: 1.5246\n"
     ]
    }
   ],
   "source": [
    "y_train_binary = (y_train == 0).astype(int)  # 1 for 'Breast Invasive Ductal Carcinoma', 0 otherwise\n",
    "y_valid_binary = (y_valid == 0).astype(int)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Define class weights for balancing\n",
    "class_weights = {\n",
    "    0: 7,  # for 'Other Classes'\n",
    "    1: 1   # for 'Breast Invasive Ductal Carcinoma'\n",
    "}\n",
    "\n",
    "# Model construction with simplified architecture\n",
    "model = Sequential([\n",
    "    Dense(250, activation='relu'),\n",
    "    Dropout(0.2),# Reduced number of neurons\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with adjusted parameters\n",
    "history = model.fit(X_train, y_train_binary, epochs=100, batch_size=200,  # Increased epochs, reduced batch size\n",
    "                    validation_data=(X_valid, y_valid_binary),\n",
    "                    class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e0fbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step\n",
      "Classification Report:\n",
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "                   Other Classes       0.41      0.43      0.42        30\n",
      "Breast Invasive Ductal Carcinoma       0.86      0.84      0.85       120\n",
      "\n",
      "                        accuracy                           0.76       150\n",
      "                       macro avg       0.63      0.64      0.63       150\n",
      "                    weighted avg       0.77      0.76      0.76       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' and 'X_valid' are already defined and model is trained\n",
    "\n",
    "# Predict the probabilities for the validation set\n",
    "y_pred_probs = model.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_valid_binary, y_pred_binary, target_names=['Other Classes', 'Breast Invasive Ductal Carcinoma'])\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a34ce120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step - accuracy: 0.5035 - loss: 4.5473 - val_accuracy: 0.7600 - val_loss: 0.4997\n",
      "Epoch 2/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5919 - loss: 1.6545 - val_accuracy: 0.6800 - val_loss: 0.5546\n",
      "Epoch 3/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6137 - loss: 1.4960 - val_accuracy: 0.7400 - val_loss: 0.5507\n",
      "Epoch 4/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7342 - loss: 1.2151 - val_accuracy: 0.5800 - val_loss: 0.7362\n",
      "Epoch 5/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7353 - loss: 1.1273 - val_accuracy: 0.8133 - val_loss: 0.4315\n",
      "Epoch 6/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7197 - loss: 1.1451 - val_accuracy: 0.9067 - val_loss: 0.2951\n",
      "Epoch 7/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8049 - loss: 0.8670 - val_accuracy: 0.8533 - val_loss: 0.3919\n",
      "Epoch 8/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8812 - loss: 0.7355 - val_accuracy: 0.7133 - val_loss: 0.5397\n",
      "Epoch 9/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7863 - loss: 0.7499 - val_accuracy: 0.8733 - val_loss: 0.3317\n",
      "Epoch 10/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8060 - loss: 0.7077 - val_accuracy: 0.9133 - val_loss: 0.2459\n",
      "Epoch 11/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8467 - loss: 0.6649 - val_accuracy: 0.9200 - val_loss: 0.2208\n",
      "Epoch 12/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9007 - loss: 0.5118 - val_accuracy: 0.8933 - val_loss: 0.3048\n",
      "Epoch 13/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9335 - loss: 0.3825 - val_accuracy: 0.8867 - val_loss: 0.3009\n",
      "Epoch 14/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9298 - loss: 0.3609 - val_accuracy: 0.9067 - val_loss: 0.2967\n",
      "Epoch 15/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9480 - loss: 0.2752 - val_accuracy: 0.9133 - val_loss: 0.3541\n",
      "Epoch 16/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9168 - loss: 0.3054 - val_accuracy: 0.9133 - val_loss: 0.3344\n",
      "Epoch 17/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9398 - loss: 0.2190 - val_accuracy: 0.9200 - val_loss: 0.3447\n",
      "Epoch 18/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9385 - loss: 0.1960 - val_accuracy: 0.9000 - val_loss: 0.3791\n",
      "Epoch 19/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9864 - loss: 0.1017 - val_accuracy: 0.8933 - val_loss: 0.4149\n",
      "Epoch 20/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9695 - loss: 0.1040 - val_accuracy: 0.9000 - val_loss: 0.4376\n",
      "Epoch 21/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9686 - loss: 0.0941 - val_accuracy: 0.9000 - val_loss: 0.4394\n",
      "Epoch 22/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9837 - loss: 0.0912 - val_accuracy: 0.9067 - val_loss: 0.4451\n",
      "Epoch 23/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9875 - loss: 0.0526 - val_accuracy: 0.9000 - val_loss: 0.4623\n",
      "Epoch 24/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9851 - loss: 0.1526 - val_accuracy: 0.9000 - val_loss: 0.4818\n",
      "Epoch 25/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9901 - loss: 0.0455 - val_accuracy: 0.8733 - val_loss: 0.5368\n",
      "Epoch 26/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9762 - loss: 0.0730 - val_accuracy: 0.9000 - val_loss: 0.5680\n",
      "Epoch 27/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9934 - loss: 0.0272 - val_accuracy: 0.9133 - val_loss: 0.5381\n",
      "Epoch 28/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9964 - loss: 0.0356 - val_accuracy: 0.9067 - val_loss: 0.5339\n",
      "Epoch 29/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9855 - loss: 0.0475 - val_accuracy: 0.9067 - val_loss: 0.5231\n",
      "Epoch 30/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9983 - loss: 0.0229 - val_accuracy: 0.9133 - val_loss: 0.5292\n",
      "Epoch 31/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9906 - loss: 0.0333 - val_accuracy: 0.9133 - val_loss: 0.5687\n",
      "Epoch 32/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9868 - loss: 0.0334 - val_accuracy: 0.9067 - val_loss: 0.6303\n",
      "Epoch 33/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9948 - loss: 0.0262 - val_accuracy: 0.9133 - val_loss: 0.6549\n",
      "Epoch 34/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9993 - loss: 0.0157 - val_accuracy: 0.9133 - val_loss: 0.6315\n",
      "Epoch 35/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9913 - loss: 0.0409 - val_accuracy: 0.9133 - val_loss: 0.6245\n",
      "Epoch 36/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9847 - loss: 0.0433 - val_accuracy: 0.8933 - val_loss: 0.6167\n",
      "Epoch 37/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0282 - val_accuracy: 0.9000 - val_loss: 0.5709\n",
      "Epoch 38/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9931 - loss: 0.0219 - val_accuracy: 0.9200 - val_loss: 0.5822\n",
      "Epoch 39/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9958 - loss: 0.0192 - val_accuracy: 0.9133 - val_loss: 0.6188\n",
      "Epoch 40/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9963 - loss: 0.0261 - val_accuracy: 0.9000 - val_loss: 0.6323\n",
      "Epoch 41/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9957 - loss: 0.0293 - val_accuracy: 0.8933 - val_loss: 0.6301\n",
      "Epoch 42/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9886 - loss: 0.0304 - val_accuracy: 0.9000 - val_loss: 0.5790\n",
      "Epoch 43/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9977 - loss: 0.0149 - val_accuracy: 0.9333 - val_loss: 0.5280\n",
      "Epoch 44/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9975 - loss: 0.0161 - val_accuracy: 0.9267 - val_loss: 0.5522\n",
      "Epoch 45/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9913 - loss: 0.0329 - val_accuracy: 0.9133 - val_loss: 0.5920\n",
      "Epoch 46/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9972 - loss: 0.0393 - val_accuracy: 0.8933 - val_loss: 0.6121\n",
      "Epoch 47/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9974 - loss: 0.0126 - val_accuracy: 0.9133 - val_loss: 0.6123\n",
      "Epoch 48/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9950 - loss: 0.0102 - val_accuracy: 0.9133 - val_loss: 0.6278\n",
      "Epoch 49/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9938 - loss: 0.0152 - val_accuracy: 0.9133 - val_loss: 0.6464\n",
      "Epoch 50/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9981 - loss: 0.0113 - val_accuracy: 0.9067 - val_loss: 0.7797\n",
      "Epoch 51/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.9067 - val_loss: 0.7706\n",
      "Epoch 52/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0142 - val_accuracy: 0.9133 - val_loss: 0.7820\n",
      "Epoch 53/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9133 - val_loss: 0.8070\n",
      "Epoch 54/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9971 - loss: 0.0103 - val_accuracy: 0.9133 - val_loss: 0.8116\n",
      "Epoch 55/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.9067 - val_loss: 0.8147\n",
      "Epoch 56/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9958 - loss: 0.0235 - val_accuracy: 0.9133 - val_loss: 0.8456\n",
      "Epoch 57/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9983 - loss: 0.0065 - val_accuracy: 0.9067 - val_loss: 0.8695\n",
      "Epoch 58/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9964 - loss: 0.0133 - val_accuracy: 0.9000 - val_loss: 0.8502\n",
      "Epoch 59/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9894 - loss: 0.0320 - val_accuracy: 0.9133 - val_loss: 0.8085\n",
      "Epoch 60/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9994 - loss: 0.0193 - val_accuracy: 0.8867 - val_loss: 0.8004\n",
      "Epoch 61/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9969 - loss: 0.0128 - val_accuracy: 0.9067 - val_loss: 0.8724\n",
      "Epoch 62/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0172 - val_accuracy: 0.8933 - val_loss: 0.9166\n",
      "Epoch 63/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9981 - loss: 0.0082 - val_accuracy: 0.9000 - val_loss: 0.9093\n",
      "Epoch 64/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9985 - loss: 0.0198 - val_accuracy: 0.9000 - val_loss: 0.8979\n",
      "Epoch 65/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9950 - loss: 0.0163 - val_accuracy: 0.9000 - val_loss: 0.8505\n",
      "Epoch 66/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 0.0238 - val_accuracy: 0.9067 - val_loss: 0.7861\n",
      "Epoch 67/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9994 - loss: 0.0054 - val_accuracy: 0.9067 - val_loss: 0.8771\n",
      "Epoch 68/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9995 - loss: 0.0046 - val_accuracy: 0.9000 - val_loss: 0.9401\n",
      "Epoch 69/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.9000 - val_loss: 0.9443\n",
      "Epoch 70/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 0.8867 - val_loss: 0.9542\n",
      "Epoch 71/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9000 - val_loss: 0.9431\n",
      "Epoch 72/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9976 - loss: 0.0070 - val_accuracy: 0.9067 - val_loss: 0.9231\n",
      "Epoch 73/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9067 - val_loss: 0.9194\n",
      "Epoch 74/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9067 - val_loss: 0.9360\n",
      "Epoch 75/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9067 - val_loss: 0.9742\n",
      "Epoch 76/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9067 - val_loss: 0.9952\n",
      "Epoch 77/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9067 - val_loss: 0.9648\n",
      "Epoch 78/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9000 - val_loss: 0.9787\n",
      "Epoch 79/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.3350e-04 - val_accuracy: 0.9000 - val_loss: 1.0024\n",
      "Epoch 80/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9000 - val_loss: 1.0187\n",
      "Epoch 81/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 8.1349e-04 - val_accuracy: 0.8933 - val_loss: 1.0090\n",
      "Epoch 82/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9000 - val_loss: 1.0112\n",
      "Epoch 83/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.8371e-04 - val_accuracy: 0.9000 - val_loss: 1.0200\n",
      "Epoch 84/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.4975e-04 - val_accuracy: 0.9000 - val_loss: 1.0207\n",
      "Epoch 85/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 0.0044 - val_accuracy: 0.9067 - val_loss: 0.9650\n",
      "Epoch 86/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9957 - loss: 0.0098 - val_accuracy: 0.8933 - val_loss: 0.9861\n",
      "Epoch 87/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0123 - val_accuracy: 0.8667 - val_loss: 1.1107\n",
      "Epoch 88/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9917 - loss: 0.0263 - val_accuracy: 0.8933 - val_loss: 1.0695\n",
      "Epoch 89/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.1712e-04 - val_accuracy: 0.9000 - val_loss: 1.0569\n",
      "Epoch 90/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.8867 - val_loss: 1.1004\n",
      "Epoch 91/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9963 - loss: 0.0099 - val_accuracy: 0.8933 - val_loss: 1.1081\n",
      "Epoch 92/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9960 - loss: 0.0258 - val_accuracy: 0.8800 - val_loss: 1.1330\n",
      "Epoch 93/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9976 - loss: 0.0055 - val_accuracy: 0.8867 - val_loss: 1.1124\n",
      "Epoch 94/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9968 - loss: 0.0240 - val_accuracy: 0.8867 - val_loss: 1.0473\n",
      "Epoch 95/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9996 - loss: 0.0089 - val_accuracy: 0.8467 - val_loss: 1.1138\n",
      "Epoch 96/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9887 - loss: 0.0371 - val_accuracy: 0.8867 - val_loss: 1.0933\n",
      "Epoch 97/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0073 - val_accuracy: 0.8667 - val_loss: 1.0884\n",
      "Epoch 98/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9976 - loss: 0.0160 - val_accuracy: 0.8800 - val_loss: 1.1105\n",
      "Epoch 99/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9976 - loss: 0.0126 - val_accuracy: 0.9000 - val_loss: 1.0567\n",
      "Epoch 100/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9000 - val_loss: 1.0388\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_train and y_valid contain class labels\n",
    "# Let's say 'Class 1' is the new focus, adapt y_train_binary and y_valid_binary accordingly\n",
    "y_train_binary = (y_resampled == 1).astype(int)  # 1 for 'Class 1', 0 for others\n",
    "y_valid_binary = (y_valid == 1).astype(int)\n",
    "\n",
    "# Update class weights if needed\n",
    "class_weight_dict = {\n",
    "    0: 1,       # Normal weight for 'Other Classes'\n",
    "    1: 6       # Increased weight for 'Class 1'\n",
    "}\n",
    "input_shape = X_train.shape[1]  # Number of input features\n",
    "\n",
    "# Construct a new model architecture\n",
    "model_class_1 = Sequential([\n",
    "    Dense(250, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(156, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the optimizer with the desired learning rate\n",
    "adam_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with the customized optimizer\n",
    "model_class_1.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the new model\n",
    "history_class_1 = model_class_1.fit(X_resampled, y_train_binary, epochs=100, batch_size=50,\n",
    "                                    validation_data=(X_valid, y_valid_binary),\n",
    "                                    class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "729fb2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023FE29A9EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 531ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023FE29A9EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step\n",
      "Classification Report for Class 1:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Other Classes       0.96      0.94      0.95       139\n",
      "      Class 1       0.36      0.45      0.40        11\n",
      "\n",
      "     accuracy                           0.90       150\n",
      "    macro avg       0.66      0.69      0.67       150\n",
      " weighted avg       0.91      0.90      0.91       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities for the validation set\n",
    "y_pred_probs_class_1 = model_class_1.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary_class_1 = (y_pred_probs_class_1 > 0.5).astype(int)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report_class_1 = classification_report(y_valid_binary, y_pred_binary_class_1, \n",
    "                                       target_names=['Other Classes', 'Class 1'])\n",
    "print(\"Classification Report for Class 1:\\n\", report_class_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c52f707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 109ms/step - accuracy: 0.5410 - loss: 0.7112 - val_accuracy: 0.7133 - val_loss: 0.6331\n",
      "Epoch 2/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5214 - loss: 0.7210 - val_accuracy: 0.6333 - val_loss: 0.6630\n",
      "Epoch 3/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5608 - loss: 0.7193 - val_accuracy: 0.5467 - val_loss: 0.6794\n",
      "Epoch 4/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5381 - loss: 0.6716 - val_accuracy: 0.5067 - val_loss: 0.6873\n",
      "Epoch 5/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5381 - loss: 0.6974 - val_accuracy: 0.5133 - val_loss: 0.6885\n",
      "Epoch 6/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5548 - loss: 0.6849 - val_accuracy: 0.5200 - val_loss: 0.6832\n",
      "Epoch 7/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5749 - loss: 0.6705 - val_accuracy: 0.4933 - val_loss: 0.6817\n",
      "Epoch 8/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5789 - loss: 0.6721 - val_accuracy: 0.4667 - val_loss: 0.6872\n",
      "Epoch 9/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5837 - loss: 0.6832 - val_accuracy: 0.4733 - val_loss: 0.6838\n",
      "Epoch 10/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6155 - loss: 0.6600 - val_accuracy: 0.4800 - val_loss: 0.6740\n",
      "Epoch 11/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5927 - loss: 0.6548 - val_accuracy: 0.4733 - val_loss: 0.6735\n",
      "Epoch 12/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6016 - loss: 0.6562 - val_accuracy: 0.4800 - val_loss: 0.6734\n",
      "Epoch 13/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6153 - loss: 0.6329 - val_accuracy: 0.4800 - val_loss: 0.6758\n",
      "Epoch 14/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6010 - loss: 0.6701 - val_accuracy: 0.4733 - val_loss: 0.6773\n",
      "Epoch 15/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6249 - loss: 0.6404 - val_accuracy: 0.4800 - val_loss: 0.6787\n",
      "Epoch 16/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5984 - loss: 0.6656 - val_accuracy: 0.4467 - val_loss: 0.6897\n",
      "Epoch 17/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6539 - loss: 0.6184 - val_accuracy: 0.4733 - val_loss: 0.6819\n",
      "Epoch 18/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6557 - loss: 0.6117 - val_accuracy: 0.4933 - val_loss: 0.6774\n",
      "Epoch 19/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6557 - loss: 0.6182 - val_accuracy: 0.4800 - val_loss: 0.6769\n",
      "Epoch 20/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6366 - loss: 0.6025 - val_accuracy: 0.4867 - val_loss: 0.6775\n",
      "Epoch 21/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6408 - loss: 0.6364 - val_accuracy: 0.4867 - val_loss: 0.6822\n",
      "Epoch 22/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6853 - loss: 0.5929 - val_accuracy: 0.4667 - val_loss: 0.6875\n",
      "Epoch 23/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6580 - loss: 0.5885 - val_accuracy: 0.5000 - val_loss: 0.6831\n",
      "Epoch 24/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6904 - loss: 0.5802 - val_accuracy: 0.5067 - val_loss: 0.6841\n",
      "Epoch 25/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6715 - loss: 0.5654 - val_accuracy: 0.5067 - val_loss: 0.6784\n",
      "Epoch 26/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7066 - loss: 0.5560 - val_accuracy: 0.5133 - val_loss: 0.6794\n",
      "Epoch 27/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6738 - loss: 0.5734 - val_accuracy: 0.4933 - val_loss: 0.6832\n",
      "Epoch 28/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7131 - loss: 0.5563 - val_accuracy: 0.5200 - val_loss: 0.6739\n",
      "Epoch 29/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7205 - loss: 0.5391 - val_accuracy: 0.5267 - val_loss: 0.6694\n",
      "Epoch 30/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6904 - loss: 0.5322 - val_accuracy: 0.5333 - val_loss: 0.6651\n",
      "Epoch 31/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6765 - loss: 0.5501 - val_accuracy: 0.5267 - val_loss: 0.6686\n",
      "Epoch 32/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7348 - loss: 0.5118 - val_accuracy: 0.5400 - val_loss: 0.6628\n",
      "Epoch 33/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7482 - loss: 0.4996 - val_accuracy: 0.5667 - val_loss: 0.6532\n",
      "Epoch 34/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7067 - loss: 0.5347 - val_accuracy: 0.5667 - val_loss: 0.6541\n",
      "Epoch 35/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7756 - loss: 0.4736 - val_accuracy: 0.5800 - val_loss: 0.6459\n",
      "Epoch 36/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7603 - loss: 0.4772 - val_accuracy: 0.6200 - val_loss: 0.6432\n",
      "Epoch 37/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7888 - loss: 0.4610 - val_accuracy: 0.5933 - val_loss: 0.6485\n",
      "Epoch 38/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8002 - loss: 0.4327 - val_accuracy: 0.6267 - val_loss: 0.6289\n",
      "Epoch 39/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7590 - loss: 0.4452 - val_accuracy: 0.6467 - val_loss: 0.6221\n",
      "Epoch 40/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7722 - loss: 0.4468 - val_accuracy: 0.6667 - val_loss: 0.6121\n",
      "Epoch 41/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7848 - loss: 0.4531 - val_accuracy: 0.6533 - val_loss: 0.6243\n",
      "Epoch 42/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8015 - loss: 0.4320 - val_accuracy: 0.6333 - val_loss: 0.6263\n",
      "Epoch 43/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8033 - loss: 0.4102 - val_accuracy: 0.6533 - val_loss: 0.6343\n",
      "Epoch 44/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7975 - loss: 0.3919 - val_accuracy: 0.6400 - val_loss: 0.6337\n",
      "Epoch 45/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7958 - loss: 0.4035 - val_accuracy: 0.6600 - val_loss: 0.6245\n",
      "Epoch 46/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7979 - loss: 0.4103 - val_accuracy: 0.6667 - val_loss: 0.6109\n",
      "Epoch 47/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8347 - loss: 0.3770 - val_accuracy: 0.6533 - val_loss: 0.6172\n",
      "Epoch 48/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8266 - loss: 0.3813 - val_accuracy: 0.6600 - val_loss: 0.6238\n",
      "Epoch 49/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8496 - loss: 0.3442 - val_accuracy: 0.6600 - val_loss: 0.6267\n",
      "Epoch 50/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8211 - loss: 0.3938 - val_accuracy: 0.6733 - val_loss: 0.6222\n",
      "Epoch 51/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8498 - loss: 0.3334 - val_accuracy: 0.6733 - val_loss: 0.6111\n",
      "Epoch 52/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8523 - loss: 0.3305 - val_accuracy: 0.6533 - val_loss: 0.6218\n",
      "Epoch 53/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8546 - loss: 0.3232 - val_accuracy: 0.6667 - val_loss: 0.6145\n",
      "Epoch 54/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8515 - loss: 0.3293 - val_accuracy: 0.6933 - val_loss: 0.6086\n",
      "Epoch 55/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8637 - loss: 0.3010 - val_accuracy: 0.6867 - val_loss: 0.6086\n",
      "Epoch 56/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8777 - loss: 0.2824 - val_accuracy: 0.6933 - val_loss: 0.6021\n",
      "Epoch 57/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8894 - loss: 0.2586 - val_accuracy: 0.7067 - val_loss: 0.6124\n",
      "Epoch 58/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8717 - loss: 0.2573 - val_accuracy: 0.6867 - val_loss: 0.6214\n",
      "Epoch 59/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8882 - loss: 0.2477 - val_accuracy: 0.7000 - val_loss: 0.6163\n",
      "Epoch 60/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8816 - loss: 0.2728 - val_accuracy: 0.7000 - val_loss: 0.6212\n",
      "Epoch 61/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8919 - loss: 0.2421 - val_accuracy: 0.7267 - val_loss: 0.6173\n",
      "Epoch 62/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9088 - loss: 0.2212 - val_accuracy: 0.7267 - val_loss: 0.6186\n",
      "Epoch 63/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9289 - loss: 0.2066 - val_accuracy: 0.7200 - val_loss: 0.6302\n",
      "Epoch 64/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9063 - loss: 0.2331 - val_accuracy: 0.7200 - val_loss: 0.6182\n",
      "Epoch 65/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9250 - loss: 0.2187 - val_accuracy: 0.7133 - val_loss: 0.6443\n",
      "Epoch 66/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9313 - loss: 0.1866 - val_accuracy: 0.6933 - val_loss: 0.6722\n",
      "Epoch 67/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9521 - loss: 0.1469 - val_accuracy: 0.7133 - val_loss: 0.6456\n",
      "Epoch 68/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9238 - loss: 0.2085 - val_accuracy: 0.7267 - val_loss: 0.6280\n",
      "Epoch 69/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9397 - loss: 0.1645 - val_accuracy: 0.7400 - val_loss: 0.6178\n",
      "Epoch 70/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9407 - loss: 0.1599 - val_accuracy: 0.7333 - val_loss: 0.6240\n",
      "Epoch 71/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9210 - loss: 0.1919 - val_accuracy: 0.7333 - val_loss: 0.6572\n",
      "Epoch 72/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9380 - loss: 0.1704 - val_accuracy: 0.7333 - val_loss: 0.6578\n",
      "Epoch 73/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9362 - loss: 0.1531 - val_accuracy: 0.7267 - val_loss: 0.6644\n",
      "Epoch 74/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9399 - loss: 0.1487 - val_accuracy: 0.7467 - val_loss: 0.6586\n",
      "Epoch 75/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9395 - loss: 0.1662 - val_accuracy: 0.7333 - val_loss: 0.7106\n",
      "Epoch 76/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9606 - loss: 0.1204 - val_accuracy: 0.7200 - val_loss: 0.7229\n",
      "Epoch 77/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9531 - loss: 0.1054 - val_accuracy: 0.7400 - val_loss: 0.7052\n",
      "Epoch 78/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9467 - loss: 0.1358 - val_accuracy: 0.7533 - val_loss: 0.6817\n",
      "Epoch 79/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9499 - loss: 0.1337 - val_accuracy: 0.7533 - val_loss: 0.6705\n",
      "Epoch 80/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9638 - loss: 0.1231 - val_accuracy: 0.7400 - val_loss: 0.7020\n",
      "Epoch 81/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9624 - loss: 0.1087 - val_accuracy: 0.7400 - val_loss: 0.7120\n",
      "Epoch 82/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9530 - loss: 0.1344 - val_accuracy: 0.7333 - val_loss: 0.7444\n",
      "Epoch 83/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9722 - loss: 0.1042 - val_accuracy: 0.7267 - val_loss: 0.7759\n",
      "Epoch 84/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9665 - loss: 0.0988 - val_accuracy: 0.7267 - val_loss: 0.7639\n",
      "Epoch 85/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9694 - loss: 0.0879 - val_accuracy: 0.7533 - val_loss: 0.7149\n",
      "Epoch 86/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.0844 - val_accuracy: 0.7467 - val_loss: 0.7393\n",
      "Epoch 87/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9581 - loss: 0.1120 - val_accuracy: 0.7467 - val_loss: 0.7537\n",
      "Epoch 88/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9701 - loss: 0.0991 - val_accuracy: 0.7400 - val_loss: 0.7974\n",
      "Epoch 89/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9719 - loss: 0.0762 - val_accuracy: 0.7467 - val_loss: 0.7850\n",
      "Epoch 90/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9747 - loss: 0.0792 - val_accuracy: 0.7533 - val_loss: 0.7796\n",
      "Epoch 91/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9781 - loss: 0.0601 - val_accuracy: 0.7400 - val_loss: 0.8306\n",
      "Epoch 92/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9739 - loss: 0.0780 - val_accuracy: 0.7333 - val_loss: 0.8392\n",
      "Epoch 93/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9800 - loss: 0.0615 - val_accuracy: 0.7467 - val_loss: 0.8138\n",
      "Epoch 94/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9750 - loss: 0.0636 - val_accuracy: 0.7600 - val_loss: 0.8004\n",
      "Epoch 95/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9844 - loss: 0.0588 - val_accuracy: 0.7333 - val_loss: 0.8454\n",
      "Epoch 96/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9744 - loss: 0.0784 - val_accuracy: 0.7533 - val_loss: 0.8530\n",
      "Epoch 97/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9816 - loss: 0.0621 - val_accuracy: 0.7200 - val_loss: 0.9062\n",
      "Epoch 98/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9750 - loss: 0.0669 - val_accuracy: 0.7200 - val_loss: 0.8875\n",
      "Epoch 99/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9742 - loss: 0.0690 - val_accuracy: 0.7333 - val_loss: 0.8942\n",
      "Epoch 100/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9862 - loss: 0.0515 - val_accuracy: 0.7533 - val_loss: 0.8741\n",
      "Epoch 101/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9837 - loss: 0.0541 - val_accuracy: 0.7200 - val_loss: 0.8991\n",
      "Epoch 102/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9748 - loss: 0.0758 - val_accuracy: 0.7200 - val_loss: 0.9217\n",
      "Epoch 103/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9815 - loss: 0.0606 - val_accuracy: 0.7200 - val_loss: 0.9258\n",
      "Epoch 104/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9747 - loss: 0.0560 - val_accuracy: 0.7533 - val_loss: 0.8542\n",
      "Epoch 105/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9884 - loss: 0.0443 - val_accuracy: 0.7600 - val_loss: 0.8565\n",
      "Epoch 106/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9846 - loss: 0.0660 - val_accuracy: 0.7600 - val_loss: 0.8886\n",
      "Epoch 107/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9836 - loss: 0.0448 - val_accuracy: 0.7600 - val_loss: 0.8745\n",
      "Epoch 108/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9791 - loss: 0.0506 - val_accuracy: 0.7600 - val_loss: 0.8982\n",
      "Epoch 109/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9863 - loss: 0.0386 - val_accuracy: 0.7733 - val_loss: 0.8730\n",
      "Epoch 110/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9786 - loss: 0.0792 - val_accuracy: 0.7733 - val_loss: 0.8873\n",
      "Epoch 111/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9832 - loss: 0.0409 - val_accuracy: 0.7400 - val_loss: 0.9680\n",
      "Epoch 112/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9900 - loss: 0.0350 - val_accuracy: 0.7467 - val_loss: 0.9455\n",
      "Epoch 113/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9828 - loss: 0.0463 - val_accuracy: 0.7533 - val_loss: 0.9588\n",
      "Epoch 114/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9871 - loss: 0.0409 - val_accuracy: 0.7600 - val_loss: 0.9775\n",
      "Epoch 115/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9847 - loss: 0.0385 - val_accuracy: 0.7600 - val_loss: 1.0053\n",
      "Epoch 116/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9750 - loss: 0.0566 - val_accuracy: 0.7533 - val_loss: 0.9984\n",
      "Epoch 117/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9934 - loss: 0.0189 - val_accuracy: 0.7600 - val_loss: 0.9763\n",
      "Epoch 118/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9890 - loss: 0.0369 - val_accuracy: 0.7533 - val_loss: 1.0120\n",
      "Epoch 119/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9947 - loss: 0.0203 - val_accuracy: 0.7467 - val_loss: 0.9929\n",
      "Epoch 120/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9898 - loss: 0.0354 - val_accuracy: 0.7400 - val_loss: 1.0203\n",
      "Validation Accuracy: 0.74, Validation Loss: 1.02\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train and y_valid contain class labels\n",
    "\n",
    "# Binarize y_train and y_valid so that 'Class 3' is 1 and all other classes are 0\n",
    "y_train_binary = (y_train == 3).astype(int)  # 1 for 'Class 3', 0 for others\n",
    "y_valid_binary = (y_valid == 3).astype(int)\n",
    "\n",
    "# Balancing the dataset with a combination of SMOTE and RandomUnderSampler\n",
    "smote = SMOTE(sampling_strategy={1: 300})  # Upsampling 'Class 3' to 300 instances\n",
    "under = RandomUnderSampler(sampling_strategy={0: 500})  # Downsampling 'Other Classes' to 500 instances\n",
    "pipeline = Pipeline([('smote', smote), ('under', under)])\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train_binary)\n",
    "\n",
    "# Recalculate class weights after resampling\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_resampled),\n",
    "    y=y_resampled\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Redesigning the neural network model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_dim=X_resampled.shape[1]),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate for better convergence\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with adjusted batch size and epochs\n",
    "history = model.fit(\n",
    "    X_resampled, y_resampled,\n",
    "    epochs=120,  # Increasing the number of epochs to allow better learning\n",
    "    batch_size=50,  # Smaller batch size for more frequent updates\n",
    "    validation_data=(X_valid, y_valid_binary),\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "# Evaluating the model's performance on the validation data\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}, Validation Loss: {val_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2cfd0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step\n",
      "Classification Report for Class 3:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Other Classes       0.89      0.80      0.85       133\n",
      "      Class 3       0.13      0.24      0.17        17\n",
      "\n",
      "     accuracy                           0.74       150\n",
      "    macro avg       0.51      0.52      0.51       150\n",
      " weighted avg       0.81      0.74      0.77       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Generating the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_valid_binary, y_pred_binary, target_names=['Other Classes', 'Class 3'])\n",
    "print(\"Classification Report for Class 3:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26797805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acfb76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ryana\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryana\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ryana\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ryana\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ryana\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: keras in c:\\users\\ryana\\anaconda3\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\ryana\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ac5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,cross_validate,StratifiedKFold\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report,roc_auc_score\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5c38041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryana\\AppData\\Local\\Temp\\ipykernel_17248\\4014827458.py:1: DtypeWarning: Columns (662,664,676,677,683,685,686,687) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cancer_type</th>\n",
       "      <th>age_at_diagnosis</th>\n",
       "      <th>cellularity</th>\n",
       "      <th>chemotherapy</th>\n",
       "      <th>pam50_+_claudin-low_subtype</th>\n",
       "      <th>cohort</th>\n",
       "      <th>er_status</th>\n",
       "      <th>neoplasm_histologic_grade</th>\n",
       "      <th>her2_status_measured_by_snp6</th>\n",
       "      <th>her2_status</th>\n",
       "      <th>...</th>\n",
       "      <th>mtap_mut</th>\n",
       "      <th>ppp2cb_mut</th>\n",
       "      <th>smarcd1_mut</th>\n",
       "      <th>nras_mut</th>\n",
       "      <th>ndfip1_mut</th>\n",
       "      <th>hras_mut</th>\n",
       "      <th>prps2_mut</th>\n",
       "      <th>smarcb1_mut</th>\n",
       "      <th>stmn2_mut</th>\n",
       "      <th>siah1_mut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>54.29</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>LumB</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>43.45</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0</td>\n",
       "      <td>LumA</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>74.11</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>LumB</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>51.87</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>LumA</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Invasive Ductal Carcinoma</td>\n",
       "      <td>87.18</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0</td>\n",
       "      <td>LumB</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GAIN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 686 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cancer_type  age_at_diagnosis cellularity  \\\n",
       "0  Breast Invasive Ductal Carcinoma             54.29        High   \n",
       "1  Breast Invasive Ductal Carcinoma             43.45    Moderate   \n",
       "2  Breast Invasive Ductal Carcinoma             74.11        High   \n",
       "3  Breast Invasive Ductal Carcinoma             51.87        High   \n",
       "4  Breast Invasive Ductal Carcinoma             87.18    Moderate   \n",
       "\n",
       "   chemotherapy pam50_+_claudin-low_subtype  cohort er_status  \\\n",
       "0             1                        LumB       1  Positive   \n",
       "1             0                        LumA       4  Positive   \n",
       "2             0                        LumB       3  Positive   \n",
       "3             0                        LumA       3  Positive   \n",
       "4             0                        LumB       1  Positive   \n",
       "\n",
       "   neoplasm_histologic_grade her2_status_measured_by_snp6 her2_status  ...  \\\n",
       "0                        3.0                      NEUTRAL    Negative  ...   \n",
       "1                        1.0                         LOSS    Negative  ...   \n",
       "2                        3.0                      NEUTRAL    Negative  ...   \n",
       "3                        2.0                      NEUTRAL    Negative  ...   \n",
       "4                        3.0                         GAIN    Positive  ...   \n",
       "\n",
       "   mtap_mut ppp2cb_mut smarcd1_mut  nras_mut  ndfip1_mut  hras_mut  prps2_mut  \\\n",
       "0         0          0           0         0           0         0          0   \n",
       "1         0          0           0         0           0         0          0   \n",
       "2         0          0           0         0           0         0          0   \n",
       "3         0          0           0         0           0         0          0   \n",
       "4         0          0           0         0           0         0          0   \n",
       "\n",
       "   smarcb1_mut stmn2_mut  siah1_mut  \n",
       "0            0         0          0  \n",
       "1            0         0          0  \n",
       "2            0         0          0  \n",
       "3            0         0          0  \n",
       "4            0         0          0  \n",
       "\n",
       "[5 rows x 686 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data_cleaned = data.drop(columns=['patient_id','er_status_measured_by_ihc'])\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62ea3f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_with_mode(data):\n",
    "    # Get a list of columns with missing values\n",
    "    columns_with_missing_values = data.columns[data.isnull().any()].tolist()\n",
    "    \n",
    "    # Iterate over each column with missing values\n",
    "    for column_name in columns_with_missing_values:\n",
    "        # Loop until no more null values in the column\n",
    "        while data[column_name].isnull().any():\n",
    "            # Iterate over rows in the DataFrame\n",
    "            for i, row in data[data[column_name].isnull()].iterrows():\n",
    "                # Filter the data for the same cancer type\n",
    "                same_type_data = data[data['cancer_type'] == row['cancer_type']]\n",
    "                \n",
    "                # Try to find 5 other entries; if fewer, take as many as available\n",
    "                if len(same_type_data) > 5:\n",
    "                    sample = same_type_data.sample(n=5)\n",
    "                else:\n",
    "                    sample = same_type_data\n",
    "                \n",
    "                # Calculate the mode of the selected sample\n",
    "                mode_value = sample[column_name].mode()\n",
    "                \n",
    "                # If mode calculation is successful and not empty, use the mode to fill the missing value\n",
    "                if not mode_value.empty:\n",
    "                    data.at[i, column_name] = mode_value.iloc[0]\n",
    "                else:\n",
    "                    # If no mode available (all values are different or no other samples), we might choose to do nothing or use a global mode\n",
    "                    # Here we're choosing to use the global mode as a fallback\n",
    "                    global_mode = data[column_name].mode()[0]\n",
    "                    data.at[i, column_name] = global_mode\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage for the column 'cellularity'\n",
    "updated_data = fill_missing_with_mode(data_cleaned)\n",
    "\n",
    "missing_values = updated_data.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Display columns with missing values and their counts\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06029b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in X after encoding: 734\n"
     ]
    }
   ],
   "source": [
    "X = data_cleaned.drop(columns=['cancer_type'])\n",
    "y = data_cleaned['cancer_type']\n",
    "\n",
    "cols_one_hot = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() <= 5]\n",
    "cols_label = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() > 5]\n",
    "\n",
    "# Initialize the transformers list for the ColumnTransformer\n",
    "transformers = []\n",
    "\n",
    "# Loop through each column in X to apply appropriate encoding\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object' or X[column].dtype == 'int':  # Adjusted to ensure we catch int types too\n",
    "        X[column] = X[column].astype(str)  # Convert everything to string to avoid mixed type errors\n",
    "        unique_values = X[column].nunique()\n",
    "        if unique_values > 5:\n",
    "            # Use LabelEncoder for columns with more than 5 unique values\n",
    "            transformers.append((column, LabelEncoder(), [column]))  # LabelEncoder usage adjusted\n",
    "        else:\n",
    "            # Use OneHotEncoder for columns with 5 or fewer unique values\n",
    "            transformers.append((column, OneHotEncoder(), [column]))\n",
    "\n",
    "# Manually apply LabelEncoder to the relevant columns before ColumnTransformer\n",
    "for name, encoder, columns in transformers:\n",
    "    if isinstance(encoder, LabelEncoder):\n",
    "        X[columns[0]] = encoder.fit_transform(X[columns[0]])  # Directly encode the column in the DataFrame\n",
    "        # Remove label encoded columns from transformer list since they are already processed\n",
    "        transformers = [(n, e, c) for n, e, c in transformers if e is not LabelEncoder]\n",
    "\n",
    "# Setup remaining transformations with OneHotEncoder using ColumnTransformer\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), [name for name, encoder, _ in transformers if isinstance(encoder, OneHotEncoder)])\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep all other columns that do not need encoding\n",
    ")\n",
    "\n",
    "# Fit and transform X with the defined ColumnTransformer for OneHotEncoder\n",
    "X_transformed = preprocessor_X.fit_transform(X)\n",
    "\n",
    "# Label encode y\n",
    "label_encoder_y = LabelEncoder()\n",
    "y_encoded = label_encoder_y.fit_transform(y)\n",
    "\n",
    "# Print the final shape of X_transformed to verify feature count\n",
    "print(\"Number of features in X after encoding:\", X_transformed.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660b1206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names and counts are consistent.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ahnak2_mut</th>\n",
       "      <th>kmt2c_mut</th>\n",
       "      <th>syne1_mut</th>\n",
       "      <th>gata3_mut</th>\n",
       "      <th>map3k1_mut</th>\n",
       "      <th>ahnak_mut</th>\n",
       "      <th>dnah11_mut</th>\n",
       "      <th>cdh1_mut</th>\n",
       "      <th>dnah2_mut</th>\n",
       "      <th>kmt2d_mut</th>\n",
       "      <th>...</th>\n",
       "      <th>spry2</th>\n",
       "      <th>srd5a1</th>\n",
       "      <th>srd5a2</th>\n",
       "      <th>srd5a3</th>\n",
       "      <th>st7</th>\n",
       "      <th>star</th>\n",
       "      <th>tnk2</th>\n",
       "      <th>tulp4</th>\n",
       "      <th>ugt2b15</th>\n",
       "      <th>ugt2b17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.160</td>\n",
       "      <td>111.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.046</td>\n",
       "      <td>76.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.056</td>\n",
       "      <td>118.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.028</td>\n",
       "      <td>220.233333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.052</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 658 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ahnak2_mut  kmt2c_mut  syne1_mut  gata3_mut  map3k1_mut   ahnak_mut  \\\n",
       "0         3.0        1.0        3.0        2.0       5.160  111.100000   \n",
       "1         1.0        1.0        0.0        7.0       2.046   76.866667   \n",
       "2         3.0        1.0        6.0        3.0       6.056  118.700000   \n",
       "3         2.0        0.0        0.0       10.0       3.028  220.233333   \n",
       "4         3.0        1.0        2.0        1.0       5.052   28.600000   \n",
       "\n",
       "   dnah11_mut  cdh1_mut  dnah2_mut  kmt2d_mut  ...  spry2  srd5a1  srd5a2  \\\n",
       "0         0.0       1.0       80.0        3.0  ...    0.0     0.0     0.0   \n",
       "1         1.0       1.0       23.0        2.0  ...    0.0     0.0     0.0   \n",
       "2         0.0       1.0       28.0        1.0  ...    0.0     0.0     0.0   \n",
       "3         1.0       1.0       14.0        1.0  ...    0.0     0.0     0.0   \n",
       "4         0.0       0.0       26.0        2.0  ...    0.0     0.0     0.0   \n",
       "\n",
       "   srd5a3  st7  star  tnk2  tulp4  ugt2b15  ugt2b17  \n",
       "0     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "1     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "2     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "3     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "4     0.0  0.0   0.0   0.0    0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 658 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Assume cols_label and cols_one_hot are defined and scoped correctly from a previous cell\n",
    "onehot_features = preprocessor_X.named_transformers_['onehot'].get_feature_names_out()\n",
    "label_features = cols_label  # Using the columns designated for label encoding\n",
    "remainder_features = [col for col in X.columns if col not in cols_one_hot and col not in cols_label]\n",
    "\n",
    "# Combining all feature names\n",
    "all_features = list(onehot_features) + label_features + remainder_features\n",
    "\n",
    "# Create the DataFrame from the transformed data\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns=all_features[:X_transformed.shape[1]])\n",
    "\n",
    "# Verify column counts and consistency\n",
    "if len(all_features[:X_transformed.shape[1]]) != X_transformed_df.shape[1]:\n",
    "    print(f\"Warning: Column count mismatch. {len(all_features[:X_transformed.shape[1]])} names for {X_transformed_df.shape[1]} actual columns.\")\n",
    "else:\n",
    "    print(\"Column names and counts are consistent.\")\n",
    "\n",
    "# Find the index of 'muc16_mut' column\n",
    "muc16_mut_index = X_transformed_df.columns.get_loc(\"muc16_mut\")\n",
    "\n",
    "# Select columns after 'muc16_mut'\n",
    "selected_columns = X_transformed_df.columns[muc16_mut_index + 1:]\n",
    "\n",
    "# Display the selected columns for the first 30 rows of the DataFrame\n",
    "display(X_transformed_df[selected_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65e5bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      brca1     brca2     palb2      pten      tp53       atm      cdh1  \\\n",
      "0 -1.211576 -0.109631 -0.594727 -1.140829  0.595648 -1.106277 -2.143642   \n",
      "1  0.427194 -0.010864 -0.736981  0.064916  0.152814  0.097883  0.103718   \n",
      "2 -0.477025 -0.151244 -0.772095 -0.847137  0.240765 -0.248629 -0.571054   \n",
      "3 -0.548368  0.201905 -0.655579 -0.891006 -0.371917  1.477406  0.814926   \n",
      "4  0.972271  0.990202 -0.241689  0.815784  0.212176  0.843807 -0.182637   \n",
      "\n",
      "      chek2       nbn       nf1  ...     spry2    srd5a1    srd5a2    srd5a3  \\\n",
      "0 -1.105545 -0.703869 -0.394923  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "1 -0.322918  0.518080 -0.696819  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "2  0.063015  0.116458  0.202719  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "3  0.903104  0.189407 -0.800239  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "4 -0.134838 -0.517778  1.051354  ... -0.057008 -0.069081 -0.062535 -0.065305   \n",
      "\n",
      "        st7      star      tnk2     tulp4   ugt2b15   ugt2b17  \n",
      "0 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "1 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "2 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "3 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "4 -0.057008 -0.061124 -0.057315 -0.052365 -0.057008 -0.052365  \n",
      "\n",
      "[5 rows x 488 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_index = X_transformed_df.columns.get_loc(\"brca1\")\n",
    "end_index = X_transformed_df.columns.get_loc(\"ugt2b17\") +1\n",
    "\n",
    "# Select the columns for scaling\n",
    "columns_to_scale = X_transformed_df.columns[start_index:end_index]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the selected columns\n",
    "X_transformed_df[columns_to_scale] = scaler.fit_transform(X_transformed_df[columns_to_scale])\n",
    "\n",
    "# Display the scaled data for these columns\n",
    "print(X_transformed_df[columns_to_scale].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5254402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (1196, 734) (1196,)\n",
      "Validation set size: (150, 734) (150,)\n",
      "Test set size: (150, 734) (150,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# X_transformed_df\n",
    "# y_encoded\n",
    "\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "    X_transformed_df, y_encoded, train_size=0.8, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train = \n",
    "\n",
    "# Second split on the remaining 20% to get 10% Validation and 10% Test\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_remaining, y_remaining, test_size=0.5, random_state=42, stratify=y_remaining\n",
    ")\n",
    "\n",
    "# X_train\n",
    "# y_train\n",
    "# X_valid\n",
    "# y_valid\n",
    "# X_test\n",
    "# y_test\n",
    "\n",
    "# Print the sizes of each dataset to confirm the splits\n",
    "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set size:\", X_valid.shape, y_valid.shape)\n",
    "print(\"Test set size:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd90917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Invasive Ductal Carcinoma             959\n",
      "Breast Mixed Ductal and Lobular Carcinoma    132\n",
      "Breast Invasive Lobular Carcinoma             91\n",
      "Breast Invasive Mixed Mucinous Carcinoma      14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train to a pandas Series for easy counting\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "# Get counts of each class\n",
    "class_counts_train = y_train_series.value_counts()\n",
    "\n",
    "# Print the counts with original class names using inverse transform of LabelEncoder\n",
    "class_names_counts_train = pd.Series(class_counts_train.index).apply(\n",
    "    lambda x: label_encoder_y.inverse_transform([x])[0])\n",
    "class_counts_train.index = class_names_counts_train\n",
    "\n",
    "# Display the class counts with names\n",
    "print(class_counts_train)\n",
    "\n",
    "# Class Number 0: Breast Invasive Ductal Carcinoma\n",
    "# Class Number 1: Breast Invasive Lobular Carcinoma\n",
    "# Class Number 2: Breast Invasive Mixed Mucinous Carcinoma\n",
    "# Class Number 3: Breast Mixed Ductal and Lobular Carcinoma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca7f19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight(\n",
    "#     class_weight='balanced',\n",
    "#     classes=np.unique(y_train),\n",
    "#     y=y_train\n",
    "# )\n",
    "# class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0873bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.layers import Dropout\n",
    "# from keras.regularizers import l2\n",
    "# from keras.layers import BatchNormalization\n",
    "# num_classes = len(np.unique(y_train))\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense(900, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Set the learning rate\n",
    "# learning_rate = 0.01  # You can change this value according to your needs\n",
    "\n",
    "# # Instantiate the optimizer with the desired learning rate\n",
    "# adam_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# # Compile the model with the customized optimizer\n",
    "# model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e2c4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=600,\n",
    "#                     validation_data=(X_valid, y_valid),\n",
    "#                     class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b47a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Summarize history for accuracy\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Summarize history for loss\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59699c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_valid)\n",
    "\n",
    "# # Convert the predicted probabilities to class labels\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# # Generate the classification report\n",
    "# report = classification_report(y_valid, y_pred_labels)\n",
    "\n",
    "# # Print the classification report\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48a974e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\ryana\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4fe4d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution: [500  91  14 132]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train contains the class labels, and X_train contains the features\n",
    "\n",
    "# Setup the RandomUnderSampler to reduce class 0 to 500 instances\n",
    "rus = RandomUnderSampler(sampling_strategy={0: 500})\n",
    "\n",
    "# Apply the resampling to the training data\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution to ensure it is as expected\n",
    "print(\"New class distribution:\", np.bincount(y_resampled))\n",
    "\n",
    "# Now X_resampled and y_resampled contain the undersampled data where class 0 has exactly 500 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d899217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 298ms/step - accuracy: 0.4756 - loss: 2.5297 - val_accuracy: 0.3267 - val_loss: 0.9448\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5170 - loss: 1.6345 - val_accuracy: 0.2733 - val_loss: 1.2464\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.3614 - loss: 1.5842 - val_accuracy: 0.6267 - val_loss: 0.6310\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6472 - loss: 1.2906 - val_accuracy: 0.2933 - val_loss: 1.2015\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.4528 - loss: 1.3142 - val_accuracy: 0.5200 - val_loss: 0.6827\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6214 - loss: 1.1631 - val_accuracy: 0.4933 - val_loss: 0.8154\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5885 - loss: 1.1376 - val_accuracy: 0.5933 - val_loss: 0.6576\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6651 - loss: 1.0241 - val_accuracy: 0.5200 - val_loss: 0.7801\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6294 - loss: 1.0871 - val_accuracy: 0.5667 - val_loss: 0.6840\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.6809 - loss: 0.9623 - val_accuracy: 0.5800 - val_loss: 0.6693\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6763 - loss: 0.9102 - val_accuracy: 0.6400 - val_loss: 0.5953\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7629 - loss: 0.8233 - val_accuracy: 0.6000 - val_loss: 0.6938\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6979 - loss: 0.8401 - val_accuracy: 0.6867 - val_loss: 0.5743\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.7842 - loss: 0.7520 - val_accuracy: 0.6867 - val_loss: 0.6195\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8284 - loss: 0.6361 - val_accuracy: 0.6600 - val_loss: 0.7229\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8005 - loss: 0.6511 - val_accuracy: 0.6733 - val_loss: 0.6304\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8281 - loss: 0.5689 - val_accuracy: 0.6733 - val_loss: 0.6560\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8627 - loss: 0.5106 - val_accuracy: 0.6533 - val_loss: 0.8020\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8597 - loss: 0.4575 - val_accuracy: 0.6133 - val_loss: 0.8614\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8173 - loss: 0.4768 - val_accuracy: 0.7067 - val_loss: 0.6700\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8959 - loss: 0.3889 - val_accuracy: 0.6933 - val_loss: 0.7502\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9237 - loss: 0.3189 - val_accuracy: 0.6533 - val_loss: 0.8955\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8877 - loss: 0.2961 - val_accuracy: 0.7000 - val_loss: 0.7785\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9190 - loss: 0.2466 - val_accuracy: 0.7133 - val_loss: 0.7816\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9573 - loss: 0.2099 - val_accuracy: 0.7067 - val_loss: 0.8472\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9516 - loss: 0.1726 - val_accuracy: 0.7200 - val_loss: 0.8860\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9582 - loss: 0.1365 - val_accuracy: 0.7400 - val_loss: 0.8794\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9792 - loss: 0.1077 - val_accuracy: 0.7333 - val_loss: 0.9223\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9800 - loss: 0.1153 - val_accuracy: 0.7067 - val_loss: 1.0565\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9747 - loss: 0.0936 - val_accuracy: 0.7200 - val_loss: 1.0359\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9664 - loss: 0.0908 - val_accuracy: 0.7133 - val_loss: 1.0501\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9936 - loss: 0.0597 - val_accuracy: 0.7000 - val_loss: 1.1052\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9903 - loss: 0.0570 - val_accuracy: 0.7200 - val_loss: 1.0881\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9836 - loss: 0.0619 - val_accuracy: 0.7467 - val_loss: 1.1020\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9907 - loss: 0.0774 - val_accuracy: 0.7267 - val_loss: 1.1276\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9868 - loss: 0.0526 - val_accuracy: 0.7533 - val_loss: 1.1064\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9910 - loss: 0.0561 - val_accuracy: 0.7667 - val_loss: 1.1421\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9931 - loss: 0.0398 - val_accuracy: 0.7667 - val_loss: 1.1364\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9937 - loss: 0.0309 - val_accuracy: 0.7267 - val_loss: 1.1806\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9963 - loss: 0.0283 - val_accuracy: 0.7000 - val_loss: 1.2569\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9895 - loss: 0.0383 - val_accuracy: 0.7600 - val_loss: 1.1802\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9927 - loss: 0.0502 - val_accuracy: 0.7333 - val_loss: 1.2022\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9962 - loss: 0.0279 - val_accuracy: 0.7533 - val_loss: 1.2274\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9949 - loss: 0.0273 - val_accuracy: 0.7067 - val_loss: 1.3344\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9969 - loss: 0.0184 - val_accuracy: 0.7533 - val_loss: 1.3255\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9984 - loss: 0.0179 - val_accuracy: 0.7400 - val_loss: 1.3435\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9988 - loss: 0.0155 - val_accuracy: 0.7600 - val_loss: 1.3479\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9986 - loss: 0.0120 - val_accuracy: 0.7533 - val_loss: 1.3838\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9972 - loss: 0.0153 - val_accuracy: 0.7267 - val_loss: 1.4056\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9950 - loss: 0.0138 - val_accuracy: 0.7467 - val_loss: 1.3430\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9985 - loss: 0.0130 - val_accuracy: 0.7467 - val_loss: 1.3388\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9991 - loss: 0.0127 - val_accuracy: 0.7333 - val_loss: 1.3717\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9985 - loss: 0.0100 - val_accuracy: 0.7600 - val_loss: 1.3653\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9977 - loss: 0.0148 - val_accuracy: 0.7467 - val_loss: 1.3652\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9994 - loss: 0.0080 - val_accuracy: 0.7400 - val_loss: 1.3968\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9996 - loss: 0.0097 - val_accuracy: 0.6933 - val_loss: 1.4982\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9901 - loss: 0.0219 - val_accuracy: 0.7733 - val_loss: 1.4001\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9968 - loss: 0.0453 - val_accuracy: 0.6933 - val_loss: 1.5735\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9918 - loss: 0.0241 - val_accuracy: 0.7600 - val_loss: 1.4188\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0179 - val_accuracy: 0.7467 - val_loss: 1.4417\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9951 - loss: 0.0159 - val_accuracy: 0.7467 - val_loss: 1.4041\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0104 - val_accuracy: 0.7400 - val_loss: 1.4340\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9996 - loss: 0.0071 - val_accuracy: 0.7200 - val_loss: 1.5525\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9956 - loss: 0.0178 - val_accuracy: 0.7333 - val_loss: 1.4839\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.7400 - val_loss: 1.4491\n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9988 - loss: 0.0065 - val_accuracy: 0.7400 - val_loss: 1.4582\n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9975 - loss: 0.0084 - val_accuracy: 0.7733 - val_loss: 1.4302\n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9998 - loss: 0.0048 - val_accuracy: 0.7667 - val_loss: 1.4416\n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.7400 - val_loss: 1.4840\n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9996 - loss: 0.0048 - val_accuracy: 0.7467 - val_loss: 1.4771\n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.7533 - val_loss: 1.4992\n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.7267 - val_loss: 1.5803\n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9954 - loss: 0.0116 - val_accuracy: 0.7267 - val_loss: 1.5739\n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.7133 - val_loss: 1.6109\n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9998 - loss: 0.0047 - val_accuracy: 0.7200 - val_loss: 1.7309\n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9973 - loss: 0.0086 - val_accuracy: 0.7333 - val_loss: 1.6343\n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9979 - loss: 0.0100 - val_accuracy: 0.7400 - val_loss: 1.6029\n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7067 - val_loss: 1.6993\n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9973 - loss: 0.0072 - val_accuracy: 0.7333 - val_loss: 1.6432\n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.7533 - val_loss: 1.6491\n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9975 - loss: 0.0071 - val_accuracy: 0.7200 - val_loss: 1.6608\n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7400 - val_loss: 1.6752\n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9998 - loss: 0.0024 - val_accuracy: 0.7267 - val_loss: 1.6908\n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.7267 - val_loss: 1.6732\n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9998 - loss: 0.0033 - val_accuracy: 0.7333 - val_loss: 1.6733\n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7600 - val_loss: 1.6609\n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7467 - val_loss: 1.6708\n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9994 - loss: 0.0033 - val_accuracy: 0.7333 - val_loss: 1.6888\n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9998 - loss: 0.0020 - val_accuracy: 0.7333 - val_loss: 1.6904\n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7333 - val_loss: 1.7082\n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9994 - loss: 0.0038 - val_accuracy: 0.7467 - val_loss: 1.7032\n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9979 - loss: 0.0271 - val_accuracy: 0.7133 - val_loss: 1.7182\n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9964 - loss: 0.0109 - val_accuracy: 0.7133 - val_loss: 1.7256\n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9971 - loss: 0.0093 - val_accuracy: 0.7267 - val_loss: 1.7763\n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9991 - loss: 0.0112 - val_accuracy: 0.7000 - val_loss: 1.6990\n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9934 - loss: 0.0189 - val_accuracy: 0.7600 - val_loss: 1.5333\n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9981 - loss: 0.0144 - val_accuracy: 0.7067 - val_loss: 1.5991\n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9992 - loss: 0.0043 - val_accuracy: 0.6933 - val_loss: 1.6436\n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9988 - loss: 0.0080 - val_accuracy: 0.7667 - val_loss: 1.6460\n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9984 - loss: 0.0110 - val_accuracy: 0.7067 - val_loss: 1.6656\n"
     ]
    }
   ],
   "source": [
    "y_train_binary = (y_train == 0).astype(int)  # 1 for 'Breast Invasive Ductal Carcinoma', 0 otherwise\n",
    "y_valid_binary = (y_valid == 0).astype(int)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Define class weights for balancing\n",
    "class_weights = {\n",
    "    0: 7,  # for 'Other Classes'\n",
    "    1: 1   # for 'Breast Invasive Ductal Carcinoma'\n",
    "}\n",
    "\n",
    "# Model construction with simplified architecture\n",
    "model_class_0 = Sequential([\n",
    "    Dense(250, activation='relu'),\n",
    "    Dropout(0.2),# Reduced number of neurons\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "model_class_0.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with adjusted parameters\n",
    "history = model_class_0.fit(X_train, y_train_binary, epochs=100, batch_size=200,  # Increased epochs, reduced batch size\n",
    "                    validation_data=(X_valid, y_valid_binary),\n",
    "                    class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "773ed234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step\n",
      "Classification Report:\n",
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "                   Other Classes       0.31      0.37      0.33        30\n",
      "Breast Invasive Ductal Carcinoma       0.83      0.79      0.81       120\n",
      "\n",
      "                        accuracy                           0.71       150\n",
      "                       macro avg       0.57      0.58      0.57       150\n",
      "                    weighted avg       0.73      0.71      0.72       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' and 'X_valid' are already defined and model is trained\n",
    "\n",
    "# Predict the probabilities for the validation set\n",
    "y_pred_probs = model_class_0.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_valid_binary, y_pred_binary, target_names=['Other Classes', 'Breast Invasive Ductal Carcinoma'])\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4aa1591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 113ms/step - accuracy: 0.6143 - loss: 2.9692 - val_accuracy: 0.8867 - val_loss: 0.4234\n",
      "Epoch 2/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6407 - loss: 1.2917 - val_accuracy: 0.6267 - val_loss: 0.6668\n",
      "Epoch 3/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5845 - loss: 1.2977 - val_accuracy: 0.8667 - val_loss: 0.4755\n",
      "Epoch 4/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7225 - loss: 1.0495 - val_accuracy: 0.7800 - val_loss: 0.5764\n",
      "Epoch 5/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6972 - loss: 1.0482 - val_accuracy: 0.8067 - val_loss: 0.5072\n",
      "Epoch 6/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7333 - loss: 0.9423 - val_accuracy: 0.7600 - val_loss: 0.5254\n",
      "Epoch 7/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7633 - loss: 0.8765 - val_accuracy: 0.8533 - val_loss: 0.4123\n",
      "Epoch 8/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7797 - loss: 0.7097 - val_accuracy: 0.8733 - val_loss: 0.3745\n",
      "Epoch 9/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7679 - loss: 0.7053 - val_accuracy: 0.9133 - val_loss: 0.2592\n",
      "Epoch 10/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8760 - loss: 0.5540 - val_accuracy: 0.9067 - val_loss: 0.2927\n",
      "Epoch 11/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8477 - loss: 0.4849 - val_accuracy: 0.9067 - val_loss: 0.2238\n",
      "Epoch 12/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9288 - loss: 0.4438 - val_accuracy: 0.8800 - val_loss: 0.3565\n",
      "Epoch 13/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9124 - loss: 0.3098 - val_accuracy: 0.8800 - val_loss: 0.3477\n",
      "Epoch 14/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9040 - loss: 0.2679 - val_accuracy: 0.8800 - val_loss: 0.3219\n",
      "Epoch 15/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9319 - loss: 0.2384 - val_accuracy: 0.8800 - val_loss: 0.3217\n",
      "Epoch 16/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9559 - loss: 0.2230 - val_accuracy: 0.8733 - val_loss: 0.4256\n",
      "Epoch 17/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9685 - loss: 0.1647 - val_accuracy: 0.9000 - val_loss: 0.3686\n",
      "Epoch 18/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9636 - loss: 0.1586 - val_accuracy: 0.8933 - val_loss: 0.3686\n",
      "Epoch 19/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9792 - loss: 0.1098 - val_accuracy: 0.8867 - val_loss: 0.3698\n",
      "Epoch 20/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9826 - loss: 0.0854 - val_accuracy: 0.9000 - val_loss: 0.4162\n",
      "Epoch 21/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9778 - loss: 0.0687 - val_accuracy: 0.9000 - val_loss: 0.3987\n",
      "Epoch 22/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9946 - loss: 0.0388 - val_accuracy: 0.9000 - val_loss: 0.4565\n",
      "Epoch 23/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9792 - loss: 0.0615 - val_accuracy: 0.8933 - val_loss: 0.4039\n",
      "Epoch 24/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9889 - loss: 0.0493 - val_accuracy: 0.9000 - val_loss: 0.4190\n",
      "Epoch 25/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9836 - loss: 0.0409 - val_accuracy: 0.8933 - val_loss: 0.4552\n",
      "Epoch 26/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9769 - loss: 0.0739 - val_accuracy: 0.8867 - val_loss: 0.5082\n",
      "Epoch 27/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9946 - loss: 0.0296 - val_accuracy: 0.9000 - val_loss: 0.5032\n",
      "Epoch 28/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9833 - loss: 0.0997 - val_accuracy: 0.9200 - val_loss: 0.4549\n",
      "Epoch 29/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9913 - loss: 0.0478 - val_accuracy: 0.8867 - val_loss: 0.5312\n",
      "Epoch 30/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9807 - loss: 0.0691 - val_accuracy: 0.8867 - val_loss: 0.5117\n",
      "Epoch 31/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9930 - loss: 0.0225 - val_accuracy: 0.9067 - val_loss: 0.5322\n",
      "Epoch 32/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9891 - loss: 0.0514 - val_accuracy: 0.9000 - val_loss: 0.5504\n",
      "Epoch 33/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0193 - val_accuracy: 0.8867 - val_loss: 0.6143\n",
      "Epoch 34/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9875 - loss: 0.0324 - val_accuracy: 0.9067 - val_loss: 0.5765\n",
      "Epoch 35/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9994 - loss: 0.0146 - val_accuracy: 0.8867 - val_loss: 0.6596\n",
      "Epoch 36/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9957 - loss: 0.0215 - val_accuracy: 0.9067 - val_loss: 0.6616\n",
      "Epoch 37/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9978 - loss: 0.0324 - val_accuracy: 0.8933 - val_loss: 0.6596\n",
      "Epoch 38/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9994 - loss: 0.0080 - val_accuracy: 0.9000 - val_loss: 0.6434\n",
      "Epoch 39/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9880 - loss: 0.0245 - val_accuracy: 0.9000 - val_loss: 0.6866\n",
      "Epoch 40/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9960 - loss: 0.0247 - val_accuracy: 0.8933 - val_loss: 0.6797\n",
      "Epoch 41/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9989 - loss: 0.0100 - val_accuracy: 0.8867 - val_loss: 0.6400\n",
      "Epoch 42/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.8933 - val_loss: 0.6643\n",
      "Epoch 43/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9979 - loss: 0.0062 - val_accuracy: 0.8867 - val_loss: 0.6705\n",
      "Epoch 44/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.8867 - val_loss: 0.6791\n",
      "Epoch 45/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9986 - loss: 0.0069 - val_accuracy: 0.9000 - val_loss: 0.6819\n",
      "Epoch 46/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9067 - val_loss: 0.6958\n",
      "Epoch 47/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9996 - loss: 0.0053 - val_accuracy: 0.9000 - val_loss: 0.7065\n",
      "Epoch 48/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9067 - val_loss: 0.7128\n",
      "Epoch 49/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9067 - val_loss: 0.7291\n",
      "Epoch 50/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9000 - val_loss: 0.7376\n",
      "Epoch 51/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9000 - val_loss: 0.7701\n",
      "Epoch 52/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9067 - val_loss: 0.7704\n",
      "Epoch 53/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.8933 - val_loss: 0.7851\n",
      "Epoch 54/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9000 - val_loss: 0.8043\n",
      "Epoch 55/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9000 - val_loss: 0.8052\n",
      "Epoch 56/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.8933 - val_loss: 0.8217\n",
      "Epoch 57/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9067 - val_loss: 0.8290\n",
      "Epoch 58/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 9.8755e-04 - val_accuracy: 0.9067 - val_loss: 0.8292\n",
      "Epoch 59/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 6.9151e-04 - val_accuracy: 0.9067 - val_loss: 0.8247\n",
      "Epoch 60/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 0.9133 - val_loss: 0.8238\n",
      "Epoch 61/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9067 - val_loss: 0.8783\n",
      "Epoch 62/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9970 - loss: 0.0030 - val_accuracy: 0.9000 - val_loss: 0.8835\n",
      "Epoch 63/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.6103e-04 - val_accuracy: 0.9000 - val_loss: 0.8881\n",
      "Epoch 64/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 6.7065e-04 - val_accuracy: 0.9000 - val_loss: 0.8919\n",
      "Epoch 65/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 3.8100e-04 - val_accuracy: 0.9000 - val_loss: 0.8954\n",
      "Epoch 66/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 5.9384e-04 - val_accuracy: 0.9000 - val_loss: 0.8986\n",
      "Epoch 67/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9998 - loss: 0.0015 - val_accuracy: 0.9067 - val_loss: 0.8987\n",
      "Epoch 68/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 2.3504e-04 - val_accuracy: 0.9133 - val_loss: 0.8934\n",
      "Epoch 69/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 5.6637e-04 - val_accuracy: 0.9133 - val_loss: 0.8972\n",
      "Epoch 70/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9984 - loss: 0.0047 - val_accuracy: 0.9067 - val_loss: 0.9043\n",
      "Epoch 71/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 3.8118e-04 - val_accuracy: 0.9000 - val_loss: 0.9199\n",
      "Epoch 72/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9994 - loss: 0.0011 - val_accuracy: 0.9067 - val_loss: 0.9199\n",
      "Epoch 73/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9067 - val_loss: 0.8583\n",
      "Epoch 74/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 6.3827e-04 - val_accuracy: 0.9000 - val_loss: 0.8534\n",
      "Epoch 75/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 2.1672e-04 - val_accuracy: 0.9000 - val_loss: 0.8550\n",
      "Epoch 76/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 6.8580e-04 - val_accuracy: 0.9067 - val_loss: 0.8587\n",
      "Epoch 77/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 0.9000 - val_loss: 0.8810\n",
      "Epoch 78/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.8933 - val_loss: 0.9163\n",
      "Epoch 79/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9995 - loss: 6.2692e-04 - val_accuracy: 0.8933 - val_loss: 0.9012\n",
      "Epoch 80/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9133 - val_loss: 0.8616\n",
      "Epoch 81/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 8.5793e-04 - val_accuracy: 0.8933 - val_loss: 0.8943\n",
      "Epoch 82/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 8.7712e-04 - val_accuracy: 0.9000 - val_loss: 0.9027\n",
      "Epoch 83/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.8933 - val_loss: 0.8898\n",
      "Epoch 84/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.8800 - val_loss: 0.9789\n",
      "Epoch 85/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0017 - val_accuracy: 0.9067 - val_loss: 0.9437\n",
      "Epoch 86/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9067 - val_loss: 0.9560\n",
      "Epoch 87/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 4.0321e-04 - val_accuracy: 0.9133 - val_loss: 0.9316\n",
      "Epoch 88/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9067 - val_loss: 0.9297\n",
      "Epoch 89/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9996 - loss: 0.0030 - val_accuracy: 0.8667 - val_loss: 1.0235\n",
      "Epoch 90/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9953 - loss: 0.0205 - val_accuracy: 0.9133 - val_loss: 1.0226\n",
      "Epoch 91/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9964 - loss: 0.0219 - val_accuracy: 0.8800 - val_loss: 0.9531\n",
      "Epoch 92/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0820 - val_accuracy: 0.9000 - val_loss: 0.5676\n",
      "Epoch 93/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9942 - loss: 0.0404 - val_accuracy: 0.8800 - val_loss: 0.6379\n",
      "Epoch 94/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9789 - loss: 0.0626 - val_accuracy: 0.8733 - val_loss: 0.8346\n",
      "Epoch 95/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0481 - val_accuracy: 0.8733 - val_loss: 0.7730\n",
      "Epoch 96/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9949 - loss: 0.0188 - val_accuracy: 0.8867 - val_loss: 0.7731\n",
      "Epoch 97/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0116 - val_accuracy: 0.8733 - val_loss: 0.9300\n",
      "Epoch 98/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9904 - loss: 0.0329 - val_accuracy: 0.9067 - val_loss: 0.7859\n",
      "Epoch 99/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.9067 - val_loss: 0.8029\n",
      "Epoch 100/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9976 - loss: 0.0063 - val_accuracy: 0.9133 - val_loss: 0.8284\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_train and y_valid contain class labels\n",
    "# Let's say 'Class 1' is the new focus, adapt y_train_binary and y_valid_binary accordingly\n",
    "y_train_binary = (y_resampled == 1).astype(int)  # 1 for 'Class 1', 0 for others\n",
    "y_valid_binary = (y_valid == 1).astype(int)\n",
    "\n",
    "# Update class weights if needed\n",
    "class_weight_dict = {\n",
    "    0: 1,       # Normal weight for 'Other Classes'\n",
    "    1: 6       # Increased weight for 'Class 1'\n",
    "}\n",
    "input_shape = X_train.shape[1]  # Number of input features\n",
    "\n",
    "# Construct a new model architecture\n",
    "model_class_1 = Sequential([\n",
    "    Dense(250, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(156, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the optimizer with the desired learning rate\n",
    "adam_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with the customized optimizer\n",
    "model_class_1.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the new model\n",
    "history_class_1 = model_class_1.fit(X_resampled, y_train_binary, epochs=100, batch_size=50,\n",
    "                                    validation_data=(X_valid, y_valid_binary),\n",
    "                                    class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d43c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step\n",
      "Classification Report for Class 1:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Other Classes       0.96      0.95      0.95       139\n",
      "      Class 1       0.42      0.45      0.43        11\n",
      "\n",
      "     accuracy                           0.91       150\n",
      "    macro avg       0.69      0.70      0.69       150\n",
      " weighted avg       0.92      0.91      0.92       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities for the validation set\n",
    "y_pred_probs_class_1 = model_class_1.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary_class_1 = (y_pred_probs_class_1 > 0.5).astype(int)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report_class_1 = classification_report(y_valid_binary, y_pred_binary_class_1, \n",
    "                                       target_names=['Other Classes', 'Class 1'])\n",
    "print(\"Classification Report for Class 1:\\n\", report_class_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe1f0748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 107ms/step - accuracy: 0.4292 - loss: 0.7957 - val_accuracy: 0.2000 - val_loss: 0.9261\n",
      "Epoch 2/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4376 - loss: 0.7380 - val_accuracy: 0.2533 - val_loss: 0.8348\n",
      "Epoch 3/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4563 - loss: 0.7398 - val_accuracy: 0.3533 - val_loss: 0.7692\n",
      "Epoch 4/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4859 - loss: 0.7351 - val_accuracy: 0.4133 - val_loss: 0.7374\n",
      "Epoch 5/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4806 - loss: 0.7125 - val_accuracy: 0.4867 - val_loss: 0.7135\n",
      "Epoch 6/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4918 - loss: 0.7133 - val_accuracy: 0.5267 - val_loss: 0.6960\n",
      "Epoch 7/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5178 - loss: 0.6756 - val_accuracy: 0.5733 - val_loss: 0.6822\n",
      "Epoch 8/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5537 - loss: 0.6860 - val_accuracy: 0.5933 - val_loss: 0.6723\n",
      "Epoch 9/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5336 - loss: 0.6849 - val_accuracy: 0.6467 - val_loss: 0.6618\n",
      "Epoch 10/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6163 - loss: 0.6685 - val_accuracy: 0.6400 - val_loss: 0.6610\n",
      "Epoch 11/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6023 - loss: 0.6713 - val_accuracy: 0.6333 - val_loss: 0.6581\n",
      "Epoch 12/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5900 - loss: 0.6831 - val_accuracy: 0.6067 - val_loss: 0.6592\n",
      "Epoch 13/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6050 - loss: 0.6604 - val_accuracy: 0.5933 - val_loss: 0.6551\n",
      "Epoch 14/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6633 - loss: 0.6414 - val_accuracy: 0.6267 - val_loss: 0.6500\n",
      "Epoch 15/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5954 - loss: 0.6731 - val_accuracy: 0.6267 - val_loss: 0.6468\n",
      "Epoch 16/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6036 - loss: 0.6545 - val_accuracy: 0.6200 - val_loss: 0.6428\n",
      "Epoch 17/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6353 - loss: 0.6452 - val_accuracy: 0.6267 - val_loss: 0.6410\n",
      "Epoch 18/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6673 - loss: 0.6013 - val_accuracy: 0.6400 - val_loss: 0.6383\n",
      "Epoch 19/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6870 - loss: 0.6072 - val_accuracy: 0.6067 - val_loss: 0.6405\n",
      "Epoch 20/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6526 - loss: 0.6134 - val_accuracy: 0.6000 - val_loss: 0.6464\n",
      "Epoch 21/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6768 - loss: 0.6066 - val_accuracy: 0.5933 - val_loss: 0.6459\n",
      "Epoch 22/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6431 - loss: 0.6065 - val_accuracy: 0.5800 - val_loss: 0.6454\n",
      "Epoch 23/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6568 - loss: 0.5891 - val_accuracy: 0.5933 - val_loss: 0.6407\n",
      "Epoch 24/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6756 - loss: 0.5946 - val_accuracy: 0.5867 - val_loss: 0.6381\n",
      "Epoch 25/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7167 - loss: 0.5514 - val_accuracy: 0.5667 - val_loss: 0.6379\n",
      "Epoch 26/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7195 - loss: 0.5545 - val_accuracy: 0.5600 - val_loss: 0.6400\n",
      "Epoch 27/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7055 - loss: 0.5546 - val_accuracy: 0.5800 - val_loss: 0.6365\n",
      "Epoch 28/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6767 - loss: 0.5637 - val_accuracy: 0.5733 - val_loss: 0.6351\n",
      "Epoch 29/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7274 - loss: 0.5577 - val_accuracy: 0.5867 - val_loss: 0.6244\n",
      "Epoch 30/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7295 - loss: 0.5308 - val_accuracy: 0.5800 - val_loss: 0.6187\n",
      "Epoch 31/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7262 - loss: 0.5171 - val_accuracy: 0.5933 - val_loss: 0.6138\n",
      "Epoch 32/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7343 - loss: 0.4956 - val_accuracy: 0.5933 - val_loss: 0.6064\n",
      "Epoch 33/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7805 - loss: 0.4785 - val_accuracy: 0.6000 - val_loss: 0.6044\n",
      "Epoch 34/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7295 - loss: 0.4811 - val_accuracy: 0.6000 - val_loss: 0.6063\n",
      "Epoch 35/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7347 - loss: 0.5058 - val_accuracy: 0.6000 - val_loss: 0.5977\n",
      "Epoch 36/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7937 - loss: 0.4555 - val_accuracy: 0.5933 - val_loss: 0.6047\n",
      "Epoch 37/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7827 - loss: 0.4506 - val_accuracy: 0.6067 - val_loss: 0.6060\n",
      "Epoch 38/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7981 - loss: 0.4379 - val_accuracy: 0.6333 - val_loss: 0.5926\n",
      "Epoch 39/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8046 - loss: 0.4489 - val_accuracy: 0.6333 - val_loss: 0.5970\n",
      "Epoch 40/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7845 - loss: 0.4551 - val_accuracy: 0.6600 - val_loss: 0.5767\n",
      "Epoch 41/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7644 - loss: 0.4447 - val_accuracy: 0.6733 - val_loss: 0.5821\n",
      "Epoch 42/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8345 - loss: 0.4008 - val_accuracy: 0.6800 - val_loss: 0.5845\n",
      "Epoch 43/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8314 - loss: 0.3776 - val_accuracy: 0.6733 - val_loss: 0.5735\n",
      "Epoch 44/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8270 - loss: 0.3738 - val_accuracy: 0.6867 - val_loss: 0.5636\n",
      "Epoch 45/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8622 - loss: 0.3375 - val_accuracy: 0.6933 - val_loss: 0.5650\n",
      "Epoch 46/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8704 - loss: 0.3279 - val_accuracy: 0.6933 - val_loss: 0.5803\n",
      "Epoch 47/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8522 - loss: 0.3458 - val_accuracy: 0.7000 - val_loss: 0.5667\n",
      "Epoch 48/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8453 - loss: 0.3321 - val_accuracy: 0.6933 - val_loss: 0.5553\n",
      "Epoch 49/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8681 - loss: 0.3054 - val_accuracy: 0.7000 - val_loss: 0.5660\n",
      "Epoch 50/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8526 - loss: 0.3382 - val_accuracy: 0.6867 - val_loss: 0.5869\n",
      "Epoch 51/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8979 - loss: 0.2731 - val_accuracy: 0.6933 - val_loss: 0.5998\n",
      "Epoch 52/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8918 - loss: 0.2682 - val_accuracy: 0.6933 - val_loss: 0.5864\n",
      "Epoch 53/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9001 - loss: 0.2771 - val_accuracy: 0.6933 - val_loss: 0.5938\n",
      "Epoch 54/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8932 - loss: 0.2711 - val_accuracy: 0.7200 - val_loss: 0.5799\n",
      "Epoch 55/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8887 - loss: 0.2489 - val_accuracy: 0.7267 - val_loss: 0.5724\n",
      "Epoch 56/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9039 - loss: 0.2486 - val_accuracy: 0.7267 - val_loss: 0.5762\n",
      "Epoch 57/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9141 - loss: 0.2283 - val_accuracy: 0.7267 - val_loss: 0.5792\n",
      "Epoch 58/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9072 - loss: 0.2586 - val_accuracy: 0.7267 - val_loss: 0.5881\n",
      "Epoch 59/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9388 - loss: 0.1847 - val_accuracy: 0.7333 - val_loss: 0.6034\n",
      "Epoch 60/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9229 - loss: 0.1938 - val_accuracy: 0.7333 - val_loss: 0.6073\n",
      "Epoch 61/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9370 - loss: 0.1701 - val_accuracy: 0.7267 - val_loss: 0.6098\n",
      "Epoch 62/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9256 - loss: 0.1965 - val_accuracy: 0.7400 - val_loss: 0.5936\n",
      "Epoch 63/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9377 - loss: 0.1770 - val_accuracy: 0.7400 - val_loss: 0.6029\n",
      "Epoch 64/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9363 - loss: 0.1649 - val_accuracy: 0.7267 - val_loss: 0.6455\n",
      "Epoch 65/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9231 - loss: 0.1905 - val_accuracy: 0.7267 - val_loss: 0.6427\n",
      "Epoch 66/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9413 - loss: 0.1587 - val_accuracy: 0.7400 - val_loss: 0.6228\n",
      "Epoch 67/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9473 - loss: 0.1366 - val_accuracy: 0.7333 - val_loss: 0.6487\n",
      "Epoch 68/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9521 - loss: 0.1137 - val_accuracy: 0.7400 - val_loss: 0.6334\n",
      "Epoch 69/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9427 - loss: 0.1273 - val_accuracy: 0.7400 - val_loss: 0.6525\n",
      "Epoch 70/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9399 - loss: 0.1457 - val_accuracy: 0.7333 - val_loss: 0.6937\n",
      "Epoch 71/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9586 - loss: 0.1096 - val_accuracy: 0.7200 - val_loss: 0.7114\n",
      "Epoch 72/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9653 - loss: 0.1070 - val_accuracy: 0.7467 - val_loss: 0.6974\n",
      "Epoch 73/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9644 - loss: 0.1202 - val_accuracy: 0.7467 - val_loss: 0.6855\n",
      "Epoch 74/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9531 - loss: 0.1180 - val_accuracy: 0.7533 - val_loss: 0.6890\n",
      "Epoch 75/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9540 - loss: 0.1186 - val_accuracy: 0.7600 - val_loss: 0.6833\n",
      "Epoch 76/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9559 - loss: 0.1053 - val_accuracy: 0.7533 - val_loss: 0.7326\n",
      "Epoch 77/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9663 - loss: 0.1056 - val_accuracy: 0.7467 - val_loss: 0.7399\n",
      "Epoch 78/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9689 - loss: 0.1046 - val_accuracy: 0.7533 - val_loss: 0.7336\n",
      "Epoch 79/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9679 - loss: 0.0902 - val_accuracy: 0.7467 - val_loss: 0.7302\n",
      "Epoch 80/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9736 - loss: 0.0845 - val_accuracy: 0.7533 - val_loss: 0.7354\n",
      "Epoch 81/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9594 - loss: 0.1077 - val_accuracy: 0.7600 - val_loss: 0.7480\n",
      "Epoch 82/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9776 - loss: 0.0840 - val_accuracy: 0.7600 - val_loss: 0.7633\n",
      "Epoch 83/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9760 - loss: 0.0787 - val_accuracy: 0.7467 - val_loss: 0.7874\n",
      "Epoch 84/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9660 - loss: 0.0873 - val_accuracy: 0.7467 - val_loss: 0.8271\n",
      "Epoch 85/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9887 - loss: 0.0521 - val_accuracy: 0.7267 - val_loss: 0.8852\n",
      "Epoch 86/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9698 - loss: 0.0769 - val_accuracy: 0.7200 - val_loss: 0.8725\n",
      "Epoch 87/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9650 - loss: 0.0892 - val_accuracy: 0.7467 - val_loss: 0.8850\n",
      "Epoch 88/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9783 - loss: 0.0637 - val_accuracy: 0.7533 - val_loss: 0.8748\n",
      "Epoch 89/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9888 - loss: 0.0470 - val_accuracy: 0.7467 - val_loss: 0.8929\n",
      "Epoch 90/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9803 - loss: 0.0583 - val_accuracy: 0.7400 - val_loss: 0.8725\n",
      "Epoch 91/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9795 - loss: 0.0640 - val_accuracy: 0.7467 - val_loss: 0.8768\n",
      "Epoch 92/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9849 - loss: 0.0600 - val_accuracy: 0.7133 - val_loss: 0.9517\n",
      "Epoch 93/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9809 - loss: 0.0630 - val_accuracy: 0.7533 - val_loss: 0.9306\n",
      "Epoch 94/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9852 - loss: 0.0436 - val_accuracy: 0.7600 - val_loss: 0.8866\n",
      "Epoch 95/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9860 - loss: 0.0490 - val_accuracy: 0.7533 - val_loss: 0.8813\n",
      "Epoch 96/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9816 - loss: 0.0573 - val_accuracy: 0.7533 - val_loss: 0.8693\n",
      "Epoch 97/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9879 - loss: 0.0451 - val_accuracy: 0.7600 - val_loss: 0.8838\n",
      "Epoch 98/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9542 - loss: 0.0850 - val_accuracy: 0.7533 - val_loss: 0.9199\n",
      "Epoch 99/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9866 - loss: 0.0462 - val_accuracy: 0.7400 - val_loss: 0.9101\n",
      "Epoch 100/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9925 - loss: 0.0303 - val_accuracy: 0.7467 - val_loss: 0.8678\n",
      "Epoch 101/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9867 - loss: 0.0393 - val_accuracy: 0.7467 - val_loss: 0.8754\n",
      "Epoch 102/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9811 - loss: 0.0517 - val_accuracy: 0.7400 - val_loss: 0.9049\n",
      "Epoch 103/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9843 - loss: 0.0424 - val_accuracy: 0.7467 - val_loss: 0.9133\n",
      "Epoch 104/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9855 - loss: 0.0407 - val_accuracy: 0.7600 - val_loss: 0.9089\n",
      "Epoch 105/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9757 - loss: 0.0489 - val_accuracy: 0.7600 - val_loss: 0.9096\n",
      "Epoch 106/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9847 - loss: 0.0526 - val_accuracy: 0.7467 - val_loss: 0.9180\n",
      "Epoch 107/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9841 - loss: 0.0474 - val_accuracy: 0.7533 - val_loss: 0.9140\n",
      "Epoch 108/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9879 - loss: 0.0419 - val_accuracy: 0.7533 - val_loss: 0.9376\n",
      "Epoch 109/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9890 - loss: 0.0328 - val_accuracy: 0.7600 - val_loss: 0.9269\n",
      "Epoch 110/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9968 - loss: 0.0244 - val_accuracy: 0.7533 - val_loss: 0.9357\n",
      "Epoch 111/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9891 - loss: 0.0420 - val_accuracy: 0.7533 - val_loss: 0.9313\n",
      "Epoch 112/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9866 - loss: 0.0365 - val_accuracy: 0.7333 - val_loss: 1.0095\n",
      "Epoch 113/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9816 - loss: 0.0390 - val_accuracy: 0.7533 - val_loss: 0.9683\n",
      "Epoch 114/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9836 - loss: 0.0301 - val_accuracy: 0.7600 - val_loss: 0.9371\n",
      "Epoch 115/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9794 - loss: 0.0605 - val_accuracy: 0.7733 - val_loss: 0.9544\n",
      "Epoch 116/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9839 - loss: 0.0340 - val_accuracy: 0.7800 - val_loss: 0.9083\n",
      "Epoch 117/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9946 - loss: 0.0227 - val_accuracy: 0.7800 - val_loss: 0.9239\n",
      "Epoch 118/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9898 - loss: 0.0304 - val_accuracy: 0.7733 - val_loss: 0.9416\n",
      "Epoch 119/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9922 - loss: 0.0265 - val_accuracy: 0.7667 - val_loss: 0.9701\n",
      "Epoch 120/120\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9891 - loss: 0.0306 - val_accuracy: 0.7667 - val_loss: 1.0128\n",
      "Validation Accuracy: 0.77, Validation Loss: 1.01\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train and y_valid contain class labels\n",
    "\n",
    "# Binarize y_train and y_valid so that 'Class 3' is 1 and all other classes are 0\n",
    "y_train_binary = (y_train == 3).astype(int)  # 1 for 'Class 3', 0 for others\n",
    "y_valid_binary = (y_valid == 3).astype(int)\n",
    "\n",
    "# Balancing the dataset with a combination of SMOTE and RandomUnderSampler\n",
    "smote = SMOTE(sampling_strategy={1: 300})  # Upsampling 'Class 3' to 300 instances\n",
    "under = RandomUnderSampler(sampling_strategy={0: 500})  # Downsampling 'Other Classes' to 500 instances\n",
    "pipeline = Pipeline([('smote', smote), ('under', under)])\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train_binary)\n",
    "\n",
    "# Recalculate class weights after resampling\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_resampled),\n",
    "    y=y_resampled\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Redesigning the neural network model\n",
    "model_class_3 = Sequential([\n",
    "    Dense(256, activation='relu', input_dim=X_resampled.shape[1]),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate for better convergence\n",
    "model_class_3.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with adjusted batch size and epochs\n",
    "history = model_class_3.fit(\n",
    "    X_resampled, y_resampled,\n",
    "    epochs=120,  # Increasing the number of epochs to allow better learning\n",
    "    batch_size=50,  # Smaller batch size for more frequent updates\n",
    "    validation_data=(X_valid, y_valid_binary),\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "# Evaluating the model's performance on the validation data\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}, Validation Loss: {val_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d5f0682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step\n",
      "Classification Report for Class 3:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Other Classes       0.91      0.82      0.86       133\n",
      "      Class 3       0.20      0.35      0.26        17\n",
      "\n",
      "     accuracy                           0.77       150\n",
      "    macro avg       0.55      0.59      0.56       150\n",
      " weighted avg       0.83      0.77      0.79       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_class_3.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to binary predictions with a threshold of 0.5\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Generating the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_valid_binary, y_pred_binary, target_names=['Other Classes', 'Class 3'])\n",
    "print(\"Classification Report for Class 3:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc247b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def predict_with_customization(X_test, y_test, model_0, model_1, model_3):\n",
    "    # Initialize an empty list to hold all predicted classes\n",
    "    predictions = []\n",
    "\n",
    "    # Convert test data to TensorFlow tensors and reshape\n",
    "    X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    X_test = tf.reshape(X_test, [-1, 734])  # Adjust to the correct feature size\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        try:\n",
    "            example = X_test[i]\n",
    "            example = tf.expand_dims(example, axis=0)\n",
    "\n",
    "            # Predict using all three models\n",
    "            pred_0 = model_0.predict(example)[0, 0]\n",
    "            pred_1 = model_1.predict(example)[0, 0]\n",
    "            pred_3 = model_3.predict(example)[0, 0]\n",
    "\n",
    "            # Determine all predicted classes\n",
    "            predicted_classes = determine_predicted_classes(pred_0, pred_1, pred_3)\n",
    "\n",
    "            # Assume the first class in the list as the primary class (or default to 2)\n",
    "            primary_class = predicted_classes[0]\n",
    "            predictions.append(primary_class)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {e}\")\n",
    "            predictions.append(2)  # Fallback to class 2 on error\n",
    "\n",
    "    # Convert predictions to a numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_test, predictions, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "    print(report)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print(cm)\n",
    "\n",
    "    # Optionally, you can also visualize the confusion matrix using matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],\n",
    "                yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def determine_predicted_classes(pred_0, pred_1, pred_3):\n",
    "    # Return a list of classes that meet or exceed the threshold of 0.5\n",
    "    predicted_classes = []\n",
    "    if pred_0 >= 0.5:\n",
    "        predicted_classes.append(0)\n",
    "    if pred_1 >= 0.5:\n",
    "        predicted_classes.append(1)\n",
    "    if pred_3 >= 0.5:\n",
    "        predicted_classes.append(3)\n",
    "    # If no classes were predicted, default to class 2\n",
    "    if not predicted_classes:\n",
    "        predicted_classes.append(2)\n",
    "    return predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13210ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.85      0.82      0.84       120\n",
      "     Class 1       0.33      0.17      0.22        12\n",
      "     Class 2       0.00      0.00      0.00         2\n",
      "     Class 3       0.26      0.31      0.29        16\n",
      "\n",
      "    accuracy                           0.71       150\n",
      "   macro avg       0.36      0.33      0.34       150\n",
      "weighted avg       0.74      0.71      0.72       150\n",
      "\n",
      "Confusion Matrix:\n",
      "[[99  4  5 12]\n",
      " [ 8  2  1  1]\n",
      " [ 1  0  0  1]\n",
      " [ 8  0  3  5]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_with_customization(X_test, y_test, model_class_0, model_class_1, model_class_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a59e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
